{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2fa8549",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from data_shuffling_split import *\n",
    "from features_extraction import *\n",
    "from data_preprocess import *\n",
    "from ml_modeling import *\n",
    "from configs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f41f6338",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_score(model, x_train, y_train, x_val, y_val, x_test, y_test):\n",
    "    \n",
    "    print(\"On Training set\\n\")\n",
    "    f1_score_result(model, x_train, y_train)\n",
    "    print(\"=\"*50)\n",
    "    print(\"On Validation set \\n\")\n",
    "    f1_score_result(model, x_val, y_val)\n",
    "    print(\"=\"*50)\n",
    "    print(\"On Training \\n\")\n",
    "    f1_score_result(model, x_test, y_test)\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f96fb7",
   "metadata": {},
   "source": [
    "# Tokenize All data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ade6d3cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of instances in the file are:  449033\n",
      "The number of instances in the training data after StratifiedShuffleSplit are:  440052\n",
      "The number of instances in the testing data after StratifiedShuffleSplit are:   8981\n",
      "The number of trainin instances:  440052\n",
      "The number of validation instances:  8981\n",
      "The number of trainin labels :  440052\n",
      "The number of validation labels :  8981\n",
      "Before Tokenization : \n",
      " ['Ø­Ø³Ø§Ø¨Ø´Ø®ØµÙŠ ÙˆØ´Ù† Ø¯Ø®Ù„ Ø§Ù‡Ø§Ù„ÙŠ Ø·Ø±Ø§Ø¨Ù„Ø³ ÙŠØ§ Ø¬Ø§Ù‡Ù„', 'Ø±Ø³Ø§Ù„Ù‡ ØªÙˆØ¹ÙˆÙŠÙ‡ ØªØ­Ø°ÙŠØ±ÙŠÙ‡ Ù…Ù† Ø´Ø±Ø·Ù‡ Ø§Ø¨ÙˆØ¸Ø¨ÙŠ Ù„Ù„Ø¬Ù…ÙŠØ¹ Ø¨ØªÙˆØ®ÙŠ Ø§Ù„Ø­Ø°Ø± ÙÙŠ Ø¸Ù„ Ø³ÙˆØ¡ Ø§Ù„Ø§Ø­ÙˆØ§Ù„ Ø§Ù„Ø¬ÙˆÙŠÙ‡ Ø´ÙƒØ±Ø§ Ø´Ø±Ø·Ù‡ Ø§Ø¨ÙˆØ¸Ø¨ÙŠ Ø±Ø§Ø¨Ø·ÙˆÙŠØ¨', 'ÙŠØ³Ø¹Ø¯ÙˆÙˆ Ø§Ù„Ø¯ÙƒØªÙˆÙˆØ± Ù‚Ù„ÙŠ Ø§Ù†Ø§ Ø­ØªÙƒÙÙ„ Ø¨Ø§Ù„Ù…Ø¹Ù…Ù„ ØªØªØ¹Ø¨ÙŠØ´ Ù†ÙØ³Ùƒ ğŸ˜­ â¤â¤']\n",
      "==================================================\n",
      "After Tokenization : \n",
      " [['Ø­Ø³Ø§Ø¨Ø´Ø®ØµÙŠ', 'ÙˆØ´Ù†', 'Ø¯Ø®Ù„', 'Ø§Ù‡Ø§Ù„ÙŠ', 'Ø·Ø±Ø§Ø¨Ù„Ø³', 'ÙŠØ§', 'Ø¬Ø§Ù‡Ù„'], ['Ø±Ø³Ø§Ù„Ù‡', 'ØªÙˆØ¹ÙˆÙŠÙ‡', 'ØªØ­Ø°ÙŠØ±ÙŠÙ‡', 'Ù…Ù†', 'Ø´Ø±Ø·Ù‡', 'Ø§Ø¨ÙˆØ¸Ø¨ÙŠ', 'Ù„Ù„Ø¬Ù…ÙŠØ¹', 'Ø¨ØªÙˆØ®ÙŠ', 'Ø§Ù„Ø­Ø°Ø±', 'ÙÙŠ', 'Ø¸Ù„', 'Ø³ÙˆØ¡', 'Ø§Ù„Ø§Ø­ÙˆØ§Ù„', 'Ø§Ù„Ø¬ÙˆÙŠÙ‡', 'Ø´ÙƒØ±Ø§', 'Ø´Ø±Ø·Ù‡', 'Ø§Ø¨ÙˆØ¸Ø¨ÙŠ', 'Ø±Ø§Ø¨Ø·ÙˆÙŠØ¨'], ['ÙŠØ³Ø¹Ø¯ÙˆÙˆ', 'Ø§Ù„Ø¯ÙƒØªÙˆÙˆØ±', 'Ù‚Ù„ÙŠ', 'Ø§Ù†Ø§', 'Ø­ØªÙƒÙÙ„', 'Ø¨Ø§Ù„Ù…Ø¹Ù…Ù„', 'ØªØªØ¹Ø¨ÙŠØ´', 'Ù†ÙØ³Ùƒ', 'ğŸ˜­', 'â¤â¤']]\n",
      "==================================================\n",
      "Before Tokenization : \n",
      " ['Ø­Ø³Ø§Ø¨Ø´Ø®ØµÙŠ Ø§Ø§Ø´Ø± Ø¹Ù„ÙŠ Ø§Ù„Ù‚Ù…Ø± ÙˆØ§Ù„Ø§Ø® ÙŠØ·Ø§Ù„Ø¹ Ø§ØµØ¨Ø¹ÙŠ . . Ø§Ù†Ø§ Ø§Ù‚ØµØ¯ Ø§Ù„Ù…Ø¨Ø§Ø±Ø§Ù‡ Ø§Ù„Ø§Ø®ÙŠØ±Ù‡ . . Ø³Ø¤Ø§Ù„ Ù…Ù† Ø³ÙŠØ¯ Ø§Ø³ÙŠØ§ Ù¢Ù Ù¡Ù© ØŸ Ø¨Ø³ ÙƒÙ„ Ø²Ù‚ Ù…Ø§Ø¨ÙŠ Ø§Ø¹Ø±Ù ğŸ˜† ', 'Ø­Ø³Ø§Ø¨Ø´Ø®ØµÙŠ Ø¯ÙƒØªÙˆØ±Ù†Ø§ Ø§ÙŠØ§Ù… Ø§Ù„Ø¬Ø§Ù…Ø¹Ù‡ ğŸ˜ Ù…Ø§ Ø´Ø§Ø¡ Ø§Ù„Ù„Ù‡ Ø¹Ù„ÙŠÙ‡', 'Ø­Ø³Ø§Ø¨Ø´Ø®ØµÙŠ ÙŠÙ…ÙƒÙ† Ø§Ù„Ø´ÙˆØ· Ø§Ù„Ø«Ø§Ù†ÙŠ Ù†Ø´ÙˆÙÙ‡']\n",
      "==================================================\n",
      "After Tokenization : \n",
      " [['Ø­Ø³Ø§Ø¨Ø´Ø®ØµÙŠ', 'Ø§Ø§Ø´Ø±', 'Ø¹Ù„ÙŠ', 'Ø§Ù„Ù‚Ù…Ø±', 'ÙˆØ§Ù„Ø§Ø®', 'ÙŠØ·Ø§Ù„Ø¹', 'Ø§ØµØ¨Ø¹ÙŠ', '.', '.', 'Ø§Ù†Ø§', 'Ø§Ù‚ØµØ¯', 'Ø§Ù„Ù…Ø¨Ø§Ø±Ø§Ù‡', 'Ø§Ù„Ø§Ø®ÙŠØ±Ù‡', '.', '.', 'Ø³Ø¤Ø§Ù„', 'Ù…Ù†', 'Ø³ÙŠØ¯', 'Ø§Ø³ÙŠØ§', 'Ù¢Ù Ù¡Ù©', 'ØŸ', 'Ø¨Ø³', 'ÙƒÙ„', 'Ø²Ù‚', 'Ù…Ø§Ø¨ÙŠ', 'Ø§Ø¹Ø±Ù', 'ğŸ˜†'], ['Ø­Ø³Ø§Ø¨Ø´Ø®ØµÙŠ', 'Ø¯ÙƒØªÙˆØ±Ù†Ø§', 'Ø§ÙŠØ§Ù…', 'Ø§Ù„Ø¬Ø§Ù…Ø¹Ù‡', 'ğŸ˜', 'Ù…Ø§', 'Ø´Ø§Ø¡', 'Ø§Ù„Ù„Ù‡', 'Ø¹Ù„ÙŠÙ‡'], ['Ø­Ø³Ø§Ø¨Ø´Ø®ØµÙŠ', 'ÙŠÙ…ÙƒÙ†', 'Ø§Ù„Ø´ÙˆØ·', 'Ø§Ù„Ø«Ø§Ù†ÙŠ', 'Ù†Ø´ÙˆÙÙ‡']]\n",
      "Before Tokenization : \n",
      " ['ÙŠØ§ÙƒØ«Ø±Ù‡Ù… ÙÙŠ Ø²Ù…Ø§Ù†Ùƒ ÙˆÙŠØ§Ù‚Ù„Ù‡Ù… ÙÙŠ ÙˆÙØ§Ùƒ : ÙŠØ§Ù…Ø§ Ø³Ù…Ø¹Ù†Ø§ Ø§Ù„Ù‚ØµØ§ÙŠØ¯ Ù„ÙƒÙ†Ù‡Ø§ Ù…Ø§ ØªÙÙŠØ¯ Ø§Ù„Ø¹Ø°Ø± ÙˆØ§Ù„ØºØ¯Ø± ÙˆØ§Ø¶Ø­ ØªØ¬Ù†ÙŠ Ø·Ø±ÙŠÙ‚ Ø§Ù„Ù‡Ù„Ø§Ùƒ : Ø§Ù…Ø§ Ø­ÙØ¸Øª Ø§Ù„Ù…ÙˆØ§ØµÙ„ ÙˆØ§Ù„Ø§ Ø®Ø³Ø±Øª Ø§Ù„Ø±ØµÙŠØ¯', 'Ø­Ø³Ø§Ø¨Ø´Ø®ØµÙŠ Ø§Ù†Ø²ÙŠÙ† Ø§Ù†Øª Ø§Ù„Ø­ÙŠÙ† Ù…Ø§ Ø¹Ø±ÙØª Ø§Ù„ØªØ³Ø¹ÙŠØ±Ù‡ Ø¹Ø´Ø§Ù† ØªÙ‚ÙˆÙ„ Ù„Ù‡ go head Ø´Ù„ÙˆÙ† ØªØ¨ÙŠ Ø§Ù„Ø³ÙŠØ§Ø±Ù‡ ØªØ³ØªÙˆÙŠ ØŸ ', 'Ø­Ø³Ø§Ø¨Ø´Ø®ØµÙŠ Ù…Ø§Ø¹Ù†Ø¯ÙƒÙ…Ø´ Ù…Ø·Ø¨Ø® ÙÙŠ Ø­ÙˆØ´ ÙÙŠÙ‡ Ø¬Ùˆ Ø±Ø§ ğŸ˜‚ ']\n",
      "==================================================\n",
      "After Tokenization : \n",
      " [['ÙŠØ§ÙƒØ«Ø±Ù‡Ù…', 'ÙÙŠ', 'Ø²Ù…Ø§Ù†Ùƒ', 'ÙˆÙŠØ§Ù‚Ù„Ù‡Ù…', 'ÙÙŠ', 'ÙˆÙØ§Ùƒ', ':', 'ÙŠØ§Ù…Ø§', 'Ø³Ù…Ø¹Ù†Ø§', 'Ø§Ù„Ù‚ØµØ§ÙŠØ¯', 'Ù„ÙƒÙ†Ù‡Ø§', 'Ù…Ø§', 'ØªÙÙŠØ¯', 'Ø§Ù„Ø¹Ø°Ø±', 'ÙˆØ§Ù„ØºØ¯Ø±', 'ÙˆØ§Ø¶Ø­', 'ØªØ¬Ù†ÙŠ', 'Ø·Ø±ÙŠÙ‚', 'Ø§Ù„Ù‡Ù„Ø§Ùƒ', ':', 'Ø§Ù…Ø§', 'Ø­ÙØ¸Øª', 'Ø§Ù„Ù…ÙˆØ§ØµÙ„', 'ÙˆØ§Ù„Ø§', 'Ø®Ø³Ø±Øª', 'Ø§Ù„Ø±ØµÙŠØ¯'], ['Ø­Ø³Ø§Ø¨Ø´Ø®ØµÙŠ', 'Ø§Ù†Ø²ÙŠÙ†', 'Ø§Ù†Øª', 'Ø§Ù„Ø­ÙŠÙ†', 'Ù…Ø§', 'Ø¹Ø±ÙØª', 'Ø§Ù„ØªØ³Ø¹ÙŠØ±Ù‡', 'Ø¹Ø´Ø§Ù†', 'ØªÙ‚ÙˆÙ„', 'Ù„Ù‡', 'go', 'head', 'Ø´Ù„ÙˆÙ†', 'ØªØ¨ÙŠ', 'Ø§Ù„Ø³ÙŠØ§Ø±Ù‡', 'ØªØ³ØªÙˆÙŠ', 'ØŸ'], ['Ø­Ø³Ø§Ø¨Ø´Ø®ØµÙŠ', 'Ù…Ø§Ø¹Ù†Ø¯ÙƒÙ…Ø´', 'Ù…Ø·Ø¨Ø®', 'ÙÙŠ', 'Ø­ÙˆØ´', 'ÙÙŠÙ‡', 'Ø¬Ùˆ', 'Ø±Ø§', 'ğŸ˜‚']]\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Train and Validation data\n",
    "strat_train_set = read_csv(\"train/strat_train_set.csv\")\n",
    "x_train_text, x_val_text, y_train, y_val = prepare_data(strat_train_set)\n",
    "\n",
    "# Test\n",
    "strat_test_set = pd.read_csv(\"dataset/test/strat_test_set.csv\")\n",
    "x_test_text, y_test = list(strat_test_set['text']), strat_test_set['dialect_l_encoded'].values\n",
    "\n",
    "\n",
    "x_train_text_tokenized = tokenize_using_nltk_TreebankWordTokenizer(x_train_text)\n",
    "\n",
    "print(\"Before Tokenization : \\n\", x_train_text[:3])\n",
    "print(\"=\"*50)\n",
    "print(\"After Tokenization : \\n\", x_train_text_tokenized[:3])\n",
    "print(\"=\"*50)\n",
    "\n",
    "x_val_text_tokenized = tokenize_using_nltk_TreebankWordTokenizer(x_val_text)\n",
    "\n",
    "print(\"Before Tokenization : \\n\", x_val_text[:3])\n",
    "print(\"=\"*50)\n",
    "print(\"After Tokenization : \\n\", x_val_text_tokenized[:3])\n",
    "\n",
    "\n",
    "x_test_text_tokenized = tokenize_using_nltk_TreebankWordTokenizer(x_test_text)\n",
    "\n",
    "print(\"Before Tokenization : \\n\", x_test_text[:3])\n",
    "print(\"=\"*50)\n",
    "print(\"After Tokenization : \\n\", x_test_text_tokenized[:3])\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8352687a",
   "metadata": {},
   "source": [
    "# Abo Bakr Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbc36cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(440052, 6400)\n",
      "==================================================\n",
      "[-0.05905 -0.0757   0.4565  -0.6665  -0.597   -0.595    0.823   -1.165\n",
      "  0.8506   1.134   -0.2147  -1.182    1.329   -1.651   -2.314    1.583\n",
      " -0.129    0.3357   0.399   -0.605    1.994    1.381   -0.942    1.259\n",
      "  2.197   -0.19    -1.18    -0.93    -1.755   -0.8086   0.646   -0.9424\n",
      "  1.502    0.7266   1.449    1.867    0.3801   1.539   -0.05664  0.728\n",
      " -0.69    -0.884   -0.7812   0.823   -1.819    0.06198  0.145   -0.08875\n",
      "  1.484    0.3823 ]\n",
      "(8981, 6400)\n",
      "==================================================\n",
      "[ 0.3884   0.5273  -0.6177   0.6475  -1.007   -1.808    0.02069 -0.5015\n",
      "  0.6987   0.0622   0.2275  -0.10034 -0.9834   0.8125  -0.3647   0.1355\n",
      " -0.4746  -0.5894   0.1296   0.8384  -0.3652   0.8203  -1.062    0.7207\n",
      " -1.255   -1.192   -0.1323   0.03262  0.8306   0.2048   0.738    0.00873\n",
      " -0.3853   1.633   -0.4583  -0.8047  -1.7     -0.05905 -1.256    0.6904\n",
      " -0.4019  -0.7856   1.004    1.708    0.5195  -1.478    0.9395   0.4226\n",
      " -0.0227  -0.2229 ]\n",
      "(9164, 6400)\n",
      "==================================================\n",
      "[-0.2822  -0.1575   3.271   -5.48    -0.6846   1.536    0.1155   0.1368\n",
      " -2.66    -2.887    0.4595   0.6196  -0.5166   0.4082   0.3364  -1.882\n",
      " -2.713   -2.545    0.4863   0.1699   0.01108  1.388   -2.156    2.8\n",
      " -2.316    1.119   -0.357   -0.5176  -0.3162   0.2896   1.391   -0.1215\n",
      " -0.7866   1.626    2.725   -0.8965   0.9175  -1.507   -1.036   -0.629\n",
      " -0.4058   0.0749   1.287    2.66    -1.376   -1.145    2.055   -0.1971\n",
      " -0.327   -3.309  ]\n"
     ]
    }
   ],
   "source": [
    "number_of_features = 100\n",
    "max_len_str = 64\n",
    "\n",
    "word_to_vec_model = load_word2vec_model(\"models/word2vec/bakrianoo_unigram_cbow_model/full_uni_cbow_100_twitter.mdl\")\n",
    "\n",
    "X_train_embed_matrix = text_to_matrix_using_word2vec(word_to_vec_model, x_train_text_tokenized, max_len_str)\n",
    "X_val_embed_matrix = text_to_matrix_using_word2vec(word_to_vec_model, x_val_text_tokenized, max_len_str)\n",
    "x_test_embed_matrix = text_to_matrix_using_word2vec(word_to_vec_model, x_test_text_tokenized, max_len_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53b0d272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On Training set\n",
      "\n",
      "===================== Validate Result =====================\n",
      "F1 score is:  0.3367897430303691\n",
      "==================================================\n",
      "On Validation set \n",
      "\n",
      "===================== Validate Result =====================\n",
      "F1 score is:  0.3323683331477564\n",
      "==================================================\n",
      "On Training \n",
      "\n",
      "===================== Validate Result =====================\n",
      "F1 score is:  0.32802269751200347\n"
     ]
    }
   ],
   "source": [
    "# Test using AdaBoostClassifier \n",
    "\n",
    "model_path    = \"bakr/AdaBoostClassifier__f1_0.325_ml.sav\"\n",
    "model = pickle_load_model(\"models/ml_models/\" + model_path)\n",
    "\n",
    "_ = train_val_test_score(model, X_train_embed_matrix, y_train, X_val_embed_matrix, y_val, \n",
    "                         x_test_embed_matrix, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc55d829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On Training set\n",
      "\n",
      "===================== Validate Result =====================\n",
      "F1 score is:  0.4002072482342996\n",
      "==================================================\n",
      "On Validation set \n",
      "\n",
      "===================== Validate Result =====================\n",
      "F1 score is:  0.4007348847567086\n",
      "==================================================\n",
      "On Training \n",
      "\n",
      "===================== Validate Result =====================\n",
      "F1 score is:  0.3644696639022261\n"
     ]
    }
   ],
   "source": [
    "# Test using Logistic Regression \n",
    "model_path    = \"bakr/unigram_100d_lg_cls_model_f1_0.366.sav\"\n",
    "model = pickle_load_model(\"models/ml_models/\" + model_path)\n",
    "\n",
    "_ = train_val_test_score(model, X_train_embed_matrix, y_train, X_val_embed_matrix, y_val, \n",
    "                         x_test_embed_matrix, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a129b7",
   "metadata": {},
   "source": [
    "#  Rezk Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "165e7636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(440052, 19200)\n",
      "==================================================\n",
      "[-0.1262   0.2761   0.2466  -0.3464  -0.5044   0.216    0.2651   0.05423\n",
      " -0.3276  -0.2793   0.328    0.1699  -0.05267  0.1941   0.292    0.1654\n",
      " -0.01619 -0.428    0.411    0.0927   0.271    0.6206  -0.04764  0.04465\n",
      "  0.0863   0.06042  0.08374 -0.0927   0.05176 -0.1616  -0.4875   0.4932\n",
      "  0.1333   0.4666   0.0387  -0.19     0.05563 -0.1526   0.549    0.2966\n",
      " -0.0969  -0.345   -0.2896  -0.0667   0.12146  0.2126   0.1146  -0.4404\n",
      " -0.1198   0.2651 ]\n",
      "(8981, 19200)\n",
      "==================================================\n",
      "[-0.1262   0.2761   0.2466  -0.3464  -0.5044   0.216    0.2651   0.05423\n",
      " -0.3276  -0.2793   0.328    0.1699  -0.05267  0.1941   0.292    0.1654\n",
      " -0.01619 -0.428    0.411    0.0927   0.271    0.6206  -0.04764  0.04465\n",
      "  0.0863   0.06042  0.08374 -0.0927   0.05176 -0.1616  -0.4875   0.4932\n",
      "  0.1333   0.4666   0.0387  -0.19     0.05563 -0.1526   0.549    0.2966\n",
      " -0.0969  -0.345   -0.2896  -0.0667   0.12146  0.2126   0.1146  -0.4404\n",
      " -0.1198   0.2651 ]\n",
      "(9164, 19200)\n",
      "==================================================\n",
      "[ 0.629     1.157    -0.6753    0.2048    0.2107    0.516    -0.6455\n",
      "  0.669    -0.72      1.04      0.2937   -0.4473   -0.2487    0.1098\n",
      " -0.1501    0.625     0.012115  0.2568   -1.126     0.2261   -0.08954\n",
      "  0.4692    0.1472    0.4668   -0.0946    0.3938    0.1757    0.413\n",
      " -0.189     0.03577  -0.816     0.8457   -0.3623   -0.564     0.619\n",
      "  0.3655    0.714    -0.3547    0.0713    1.005     0.594     0.671\n",
      "  0.7793    0.1586    0.2812   -0.11224  -0.7334    0.2106    0.4324\n",
      "  0.941   ]\n"
     ]
    }
   ],
   "source": [
    "number_of_features = 100\n",
    "max_len_str = 64\n",
    "\n",
    "word2vec_path = \"rezk_unigram_CBOW_model/train_word2vec_cbow__window_3_min_count_300\"\n",
    "word_to_vec_model = load_word2vec_model(\"models/word2vec/\" + word2vec_path)\n",
    "\n",
    "X_train_embed_matrix = text_to_matrix_using_word2vec(word_to_vec_model, x_train_text_tokenized, max_len_str)\n",
    "X_val_embed_matrix = text_to_matrix_using_word2vec(word_to_vec_model, x_val_text_tokenized, max_len_str)\n",
    "x_test_embed_matrix = text_to_matrix_using_word2vec(word_to_vec_model, x_test_text_tokenized, max_len_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c244353a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On Training set\n",
      "\n",
      "===================== Validate Result =====================\n",
      "F1 score is:  0.5133302427894885\n",
      "==================================================\n",
      "On Validation set \n",
      "\n",
      "===================== Validate Result =====================\n",
      "F1 score is:  0.5129718294176595\n",
      "==================================================\n",
      "On Training \n",
      "\n",
      "===================== Validate Result =====================\n",
      "F1 score is:  0.4143387167175906\n"
     ]
    }
   ],
   "source": [
    "# Test using LogisticRegression \n",
    "\n",
    "model_path    = \"rezk/LogisticRegression__f1_0.41_ml.sav\"\n",
    "model = pickle_load_model(\"models/ml_models/\" + model_path)\n",
    "\n",
    "_ = train_val_test_score(model, X_train_embed_matrix, y_train, X_val_embed_matrix, y_val, \n",
    "                         x_test_embed_matrix, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d400b0",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "If we compare the model prediction to the human prediction, we may conclude that the tasks of predict the dialect is difficult task for human. how its compare to the model. maybe its simpler if we talked about voice recogniation, as new layers comes in the voice it self and how people speak, the phonems, and others, its not as the same as the text.\n",
    "\n",
    "And as in the reference paper the SVC model have over **50%**, but we need more resource to train and wait for the model.\n",
    "\n",
    "Also I would like to use AdaBoostClassifier at the end with the API, as if we compared to human prediction of the text, its the best ones as we have very small gap between training and validation, and its doing the same for testing data which never seeing by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abf95d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
