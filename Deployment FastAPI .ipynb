{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39058f6b",
   "metadata": {},
   "source": [
    "# Used Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "100296d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main libraries \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from fastapi.encoders import jsonable_encoder\n",
    "from pydantic import BaseModel\n",
    "from fastapi import FastAPI\n",
    "from enum import Enum\n",
    "import pandas as pd\n",
    "import nest_asyncio\n",
    "import uvicorn\n",
    "import os\n",
    "import io\n",
    "\n",
    "# Our files\n",
    "from data_shuffling_split import *\n",
    "from features_extraction import *\n",
    "from data_preprocess import *\n",
    "from ml_modeling import *\n",
    "from configs import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6dbdda2",
   "metadata": {},
   "source": [
    "# Note !\n",
    "\n",
    "Based on what we got from **Compare ML Models**, we end up by decide of using *Abo Bakr Word2vec model*, alongside with AdaBoost Classifier.\n",
    "\n",
    "**We need to map the label as it numbers into corresponding class delicate do we use LabelEncoder as before**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51373dac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['AE', 'BH', 'DZ', 'EG', 'IQ', 'JO', 'KW', 'LB', 'LY', 'MA', 'OM',\n",
       "       'PL', 'QA', 'SA', 'SD', 'SY', 'TN', 'YE'], dtype='<U2')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_features = 100\n",
    "max_len_str  = 64\n",
    "word2vec_path = \"bakrianoo_unigram_cbow_model/full_uni_cbow_100_twitter.mdl\"\n",
    "word_to_vec_model = load_word2vec_model(\"models/word2vec/\" + word2vec_path)\n",
    "\n",
    "strat_test_set = pd.read_csv(\"dataset/test/strat_test_set.csv\")\n",
    "l_encoder = LabelEncoder()\n",
    "l_encoder.fit(list(strat_test_set[\"dialect\"]))\n",
    "l_encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3c1c8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_text(text, choosed_model, word_to_vec_model=word_to_vec_model, \n",
    "                  max_len_str=max_len_str, l_encoder=l_encoder):\n",
    "    '''\n",
    "    The function used to predict the tweets requested by the API.\n",
    "    \n",
    "    Argumen\n",
    "        text              : string, the text we need to classify.\n",
    "        word_to_vec_model : word2vec object, our word embedding model.\n",
    "        max_len_str       : integer, The maximum number of tokens represent each text.\n",
    "        l_encoder         : object, the encode of each class into some label to retrieve the class from.\n",
    "    Return\n",
    "        classifed_text    : dict, the returned classified text into the api request.\n",
    "    '''\n",
    "    # First process the text requested\n",
    "    text_cleaned = clean_text(text)\n",
    "    # Second tokenize that text to get the representation of each token\n",
    "    tokenized_text = tokenize_using_nltk_TreebankWordTokenizer([text_cleaned])\n",
    "    # Retrieve the whole text representation\n",
    "    text_features = text_to_matrix_using_word2vec(word_to_vec_model, tokenized_text, max_len_str)\n",
    "    \n",
    "    # No prediction yet\n",
    "    predicted = ''\n",
    "    # Prediction with chosen ML Model\n",
    "    if choosed_model == \"Machine Learning Model\":\n",
    "        model_path    = \"bakr/AdaBoostClassifier__f1_0.325_ml.sav\"\n",
    "        cls_model     = pickle_load_model(\"models/ml_models/\" + model_path)\n",
    "        \n",
    "        # it return a list with one index to retrieve it directly\n",
    "        predicted = cls_model.predict(text_features)\n",
    "    elif choosed_model == \"Deep Learning Model\":\n",
    "        predicted = cls_model.predict(text_features)\n",
    "        \n",
    "    \n",
    "    pred_result = l_encoder.inverse_transform(predicted)\n",
    "    # Return dictionary with request text and the predicted class\n",
    "    classifed_text = {\n",
    "        'query': text,\n",
    "        'predicted_class': str(pred_result[0])\n",
    "    }\n",
    "    return classifed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44536980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign an instance of the FastAPI class to the variable \"app\" to interact with the api.\n",
    "app = FastAPI(title='Deploying a ML & DL Model with FastAPI')\n",
    "\n",
    "# List available models using Enum\n",
    "class Model(str, Enum):\n",
    "    \"\"\"The class to choose from the models we have, Enum represent fixed value that can not change\"\"\"\n",
    "    ML_Model = \"Machine Learning Model\"\n",
    "    DL_Model = \"Deep Learning Model\"\n",
    "\n",
    "\n",
    "class Text(BaseModel):\n",
    "    \"\"\"\n",
    "    The class used to get the text from the user, \n",
    "    \"\"\"\n",
    "    user_attention: str = \"\"\"Put your Arabic text in empty value of key \"query\" below to predict it and choose which model architecture you need.\"\"\"\n",
    "    query         : str = ''\n",
    "        \n",
    "\n",
    "@app.get(\"/\")\n",
    "def home():\n",
    "    # Once you go to this link you will see the get and post method below to trying out\n",
    "    return \"Congratulations! Your API is working as expected. Now head over to http://localhost:8000/docs.\"\n",
    "\n",
    "\n",
    "# This endpoint handles all the logic necessary for the object detection to work.\n",
    "# It requires the desired model and the dictionary of tweet and default class as we give default values to us\n",
    "# In the api you can try other tweet from some_tweets below\n",
    "@app.post(\"/predict\") \n",
    "def prediction(model: Model, text_dict: Text):\n",
    "    \n",
    "    # Encode the retrived request data \n",
    "    text_dict      = jsonable_encoder(text_dict)\n",
    "    # Run our model\n",
    "    try:\n",
    "        classifed_text = classify_text(text_dict['query'], model)\n",
    "    except:\n",
    "        classifed_text = {\n",
    "        'Error': \"Please Provide us with Arabic text in empty value of key 'query' in the Request body, Thanks ^^.\"\n",
    "        }\n",
    "    return classifed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095039d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [13184]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:42866 - \"GET /docs HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:42866 - \"GET /openapi.json HTTP/1.1\" 200 OK\n",
      "\n",
      "\n",
      "[[]]\n",
      "==================================================\n",
      "==================================================\n",
      "(1, 64)\n",
      "INFO:     127.0.0.1:42866 - \"POST /predict?model=Machine%20Learning%20Model HTTP/1.1\" 200 OK\n",
      "شكرا ده شىء جميل جدا\n",
      "شكرا ده شيء جميل جدا\n",
      "[['شكرا', 'ده', 'شيء', 'جميل', 'جدا']]\n",
      "==================================================\n",
      "==================================================\n",
      "(1, 64, 100)\n",
      "(1, 6400)\n",
      "==================================================\n",
      "[ 2.857  -1.712   2.434   0.128   0.3936 -2.193   0.3833  0.055   1.605\n",
      " -4.23   -1.247  -4.336   1.614  -0.772  -1.229  -0.1295 -1.25    0.635\n",
      "  3.434  -2.48   -1.474  -2.334  -4.305   0.286  -0.6567 -0.7783 -1.347\n",
      " -1.537   2.867  -1.22   -1.742   0.4807  1.909  -3.543  -1.9795 -0.507\n",
      " -1.003   6.83   -0.4705 -1.908  -4.246   3.383   0.3538  1.767  -0.9907\n",
      "  1.106   2.25    1.004   0.3086 -1.436 ]\n",
      "==================================================\n",
      "['EG']\n",
      "INFO:     127.0.0.1:42868 - \"POST /predict?model=Machine%20Learning%20Model HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abdelrahman/.local/lib/python3.6/site-packages/sklearn/base.py:334: UserWarning: Trying to unpickle estimator LinearSVC from version 1.0.2 when using version 0.23.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/home/abdelrahman/.local/lib/python3.6/site-packages/sklearn/base.py:334: UserWarning: Trying to unpickle estimator AdaBoostClassifier from version 1.0.2 when using version 0.23.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "هذا الحساب أكثر روعة وأنت متصل على شبكتنا.\n",
      "هذا الحساب اكثر روعه وانت متصل علي شبكتنا . \n",
      "[['هذا', 'الحساب', 'اكثر', 'روعه', 'وانت', 'متصل', 'علي', 'شبكتنا', '.']]\n",
      "==================================================\n",
      "==================================================\n",
      "(1, 64, 100)\n",
      "(1, 6400)\n",
      "==================================================\n",
      "[ 0.6406  -1.85     0.01985 -0.8535   0.3623  -2.416   -1.094   -2.168\n",
      "  2.006   -2.81     1.923    0.06726  2.766   -3.68    -1.073   -1.608\n",
      " -0.6113   1.991    2.984    2.396   -2.871    0.1931   0.777    3.166\n",
      "  3.594    1.499   -0.5244   2.85     1.408   -0.753    0.717   -2.047\n",
      " -0.5664   3.615   -0.06015  3.229   -2.357    4.137    1.76    -3.941\n",
      " -3.717   -0.2527  -0.2693  -2.22     1.606    3.387   -0.11206  1.086\n",
      "  2.904   -1.972  ]\n",
      "==================================================\n",
      "['KW']\n",
      "INFO:     127.0.0.1:42910 - \"POST /predict?model=Machine%20Learning%20Model HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abdelrahman/.local/lib/python3.6/site-packages/sklearn/base.py:334: UserWarning: Trying to unpickle estimator LinearSVC from version 1.0.2 when using version 0.23.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/home/abdelrahman/.local/lib/python3.6/site-packages/sklearn/base.py:334: UserWarning: Trying to unpickle estimator AdaBoostClassifier from version 1.0.2 when using version 0.23.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# Allows the server to be run in this interactive environment\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Host depends on the setup you selected\n",
    "host = \"0.0.0.0\"\n",
    "\n",
    "# uvicorn is fast Asynchronous Server Gateway Interface (ASGI) uvicorn handles the serving\n",
    "# Spin up the server!    \n",
    "uvicorn.run(app, host=host, port=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f58cd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
