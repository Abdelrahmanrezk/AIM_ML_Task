{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7ce1444",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from configs import *\n",
    "from fetch_data import *\n",
    "from features_extraction import *\n",
    "from data_shuffling_split import *\n",
    "from data_preprocess import *\n",
    "from modeling import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52b125bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of instances in the file are:  449033\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>dialect</th>\n",
       "      <th>dialect_l_encoded</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1056552188082716800</td>\n",
       "      <td>LY</td>\n",
       "      <td>8</td>\n",
       "      <td>توا دوشه الكلاسيكو شن بيتمها وشن بيسكتهم وشن ب...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>891734969202114560</td>\n",
       "      <td>SY</td>\n",
       "      <td>15</td>\n",
       "      <td>حسابشخصي في احلي من الشحاطه 😂</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1110565179257954432</td>\n",
       "      <td>SD</td>\n",
       "      <td>14</td>\n",
       "      <td>حسابشخصي موهبه والله 😂 اوع تحاول تطورها تقوم م...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1172817955270340608</td>\n",
       "      <td>LB</td>\n",
       "      <td>7</td>\n",
       "      <td>حسابشخصي حسابشخصي 😂 انا صرلي عشر سنين مش مجدده...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>293253217821790208</td>\n",
       "      <td>QA</td>\n",
       "      <td>12</td>\n",
       "      <td>احلي شعور تكون باجازه وتقوم من الصبح وتمر ع ال...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id dialect  dialect_l_encoded  \\\n",
       "0  1056552188082716800      LY                  8   \n",
       "1   891734969202114560      SY                 15   \n",
       "2  1110565179257954432      SD                 14   \n",
       "3  1172817955270340608      LB                  7   \n",
       "4   293253217821790208      QA                 12   \n",
       "\n",
       "                                                text  \n",
       "0  توا دوشه الكلاسيكو شن بيتمها وشن بيسكتهم وشن ب...  \n",
       "1                     حسابشخصي في احلي من الشحاطه 😂   \n",
       "2  حسابشخصي موهبه والله 😂 اوع تحاول تطورها تقوم م...  \n",
       "3  حسابشخصي حسابشخصي 😂 انا صرلي عشر سنين مش مجدده...  \n",
       "4  احلي شعور تكون باجازه وتقوم من الصبح وتمر ع ال...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strat_train_set = read_csv(\"train/strat_train_set.csv\")\n",
    "strat_train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d443a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Tokenization : \n",
      " ['توا دوشه الكلاسيكو شن بيتمها وشن بيسكتهم وشن بي عصبنها هالدواره', 'حسابشخصي في احلي من الشحاطه 😂 ', 'حسابشخصي موهبه والله 😂 اوع تحاول تطورها تقوم ما تصحي']\n",
      "After Tokenization : \n",
      " [['توا', 'دوشه', 'الكلاسيكو', 'شن', 'بيتمها', 'وشن', 'بيسكتهم', 'وشن', 'بي', 'عصبنها', 'هالدواره'], ['حسابشخصي', 'في', 'احلي', 'من', 'الشحاطه', '😂'], ['حسابشخصي', 'موهبه', 'والله', '😂', 'اوع', 'تحاول', 'تطورها', 'تقوم', 'ما', 'تصحي']]\n"
     ]
    }
   ],
   "source": [
    "texts_list = list(strat_train_set['text'])\n",
    "print(\"Before Tokenization : \\n\", texts_list[:3])\n",
    "tokenized_texts = tokenize_using_nltk_TreebankWordTokenizer(texts_list)\n",
    "print(\"After Tokenization : \\n\", tokenized_texts[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b65ca08",
   "metadata": {},
   "source": [
    "# Curse of Dimensional & sparsity\n",
    "\n",
    "Tasks like **Computer Vision** or **Natural Language Processing** run to problem called **Curse of Dimensional**, and as we have here NLP classification problem, the number of instance are semi-large, but this not the point, the point is what we dealing with is text language, and the language are free of grammar, ritch of vocabulary and others.\n",
    "\n",
    "So to handle like these problems we need to extract features from the text, the old or classical way is using BOW (Bag of Words), and this approach run to the problem of **Curse of Dimensionality** as we will have number of features related to the unique words in our data. Not just that most of these features are zeros, what is we called sparse matrix.\n",
    "\n",
    "Beside of that, this matrix we will get from that approach represent the text not the word itself, so there is no similarity between words and other problem.\n",
    "\n",
    "# Word2Vec\n",
    "\n",
    "From what we have of these problem we moved to another approach related to the word representation.\n",
    "\n",
    "Word2vec is numerical representation of dense vector for the word semantics of meaning, including the implies meaning of the word. So we can use these word representation in our text as we will see.\n",
    "\n",
    "But to train Word2Vec and got a pretty good result of word representation, it first require massive data millions of text document, and second to wait for a while for your model to train. So we use the idea of transfer learning, and use some of the pre trained Arabic Word2Vec models and download it to use in our task. \n",
    "\n",
    "**Check for more information about the models we used:** [AraVec](https://www.sciencedirect.com/science/article/pii/S1877050917321749)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5de1eb",
   "metadata": {},
   "source": [
    "# Build Matrix of Text\n",
    "\n",
    "Any ML or DL model require specific number of features (input) to dealing with, but what we have here with word2vec is word representation. So how it works for text ?\n",
    "\n",
    "We will build a matrix for each text, but we need to limit the number of words in each text, because we can not train the model with different number of words in text.\n",
    "\n",
    "# Note !\n",
    "\n",
    "We can take the maximum number of words in the longest text, but maybe for some documents its has thousand of words, so we use the graph below and other helpful method to get reasonable length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7b574d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The max length is:  94\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[11, 6, 10, 26, 19, 31, 7, 6, 6, 10]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get how many words inside each text after tokenization\n",
    "num_of_words_in_each_text = [len(text) for text in tokenized_texts]\n",
    "max_len = max(num_of_words_in_each_text)\n",
    "print(\"The max length is: \", max_len)\n",
    "num_of_words_in_each_text[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3442734c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of keys before removing are:  86\n",
      "==================================================\n",
      "The number of keys after removing some of them are:  50\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1332x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# count how many times these value repeated and sort them\n",
    "new_dicts = get_keys_that_val_gr_than_num(num_of_words_in_each_text, 1000)\n",
    "keys = list(new_dicts.keys())\n",
    "values = list(new_dicts.values())\n",
    "plt.style.use('dark_background')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(18.5, 6)\n",
    "plt.bar(range(len(new_dicts)), values, tick_label=keys)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65d666b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_vec_model = load_word2vec_model(\"models/word2vec/rezk_unigram_CBOW_model/train_word2vec_cbow__window_3_min_count_300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d05c1c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_check = tokenized_texts[:50000]\n",
    "number_of_features = 300\n",
    "max_len_str = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00b02fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of matrix (50000, 9600)\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = ML_text_to_matrix_using_word2vec(word_to_vec_model, texts_check, number_of_features, max_len_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a5993a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b338242",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(strat_train_set['dialect_l_encoded']).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36dcec8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(449033, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10c92931",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abdelrahman/.local/lib/python3.6/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 1.61 GiB for an array with shape (45000, 9600) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-ba1d273c58d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mkeras_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Downloads/ml_task/AIM_ML_Task/features_extraction.py\u001b[0m in \u001b[0;36mkeras_model\u001b[0;34m(x_train, y_train, epochs, batch_size, lr)\u001b[0m\n\u001b[1;32m     78\u001b[0m                         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m                         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                         validation_split=.1)\n\u001b[0m\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1146\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1148\u001b[0;31m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[1;32m   1149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mget_data_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1381\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_cluster_coordinator\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1382\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_ClusterCoordinatorDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1383\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1150\u001b[0;31m         model=model)\n\u001b[0m\u001b[1;32m   1151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1152\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    228\u001b[0m                **kwargs):\n\u001b[1;32m    229\u001b[0m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensorLikeDataAdapter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_tensorlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     sample_weight_modes = broadcast_sample_weight_modes(\n\u001b[1;32m    232\u001b[0m         sample_weights, sample_weight_modes)\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m_process_tensorlike\u001b[0;34m(inputs)\u001b[0m\n\u001b[1;32m   1029\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1031\u001b[0;31m   \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_convert_numpy_and_scipy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1032\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_to_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 869\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    870\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 869\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    870\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m_convert_numpy_and_scipy\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloating\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloatx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0m_is_scipy_sparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_scipy_sparse_to_sparse_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor_v2_with_dispatch\u001b[0;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[1;32m   1429\u001b[0m   \"\"\"\n\u001b[1;32m   1430\u001b[0m   return convert_to_tensor_v2(\n\u001b[0;32m-> 1431\u001b[0;31m       value, dtype=dtype, dtype_hint=dtype_hint, name=name)\n\u001b[0m\u001b[1;32m   1432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor_v2\u001b[0;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[1;32m   1439\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1440\u001b[0m       \u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype_hint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1441\u001b[0;31m       as_ref=False)\n\u001b[0m\u001b[1;32m   1442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/profiler/trace.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtrace_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1565\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1566\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1568\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/framework/tensor_conversion_registry.py\u001b[0m in \u001b[0;36m_default_conversion_function\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_default_conversion_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mdel\u001b[0m \u001b[0mas_ref\u001b[0m  \u001b[0;31m# Unused.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    270\u001b[0m   \"\"\"\n\u001b[1;32m    271\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0;32m--> 272\u001b[0;31m                         allow_broadcast=True)\n\u001b[0m\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    281\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tf.constant\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m   \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m   \u001b[0;34m\"\"\"Creates a constant on the current device.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m   \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    104\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 1.61 GiB for an array with shape (45000, 9600) and data type float32"
     ]
    }
   ],
   "source": [
    "keras_model(embedding_matrix, y_train, 10, 32, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e939bb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logreg_model = LogisticRegression(C=2.0, multi_class='ovr', penalty='l2', solver='liblinear')\n",
    "# logreg_model.fit(train_embedding_matrix, target_train)\n",
    "\n",
    "# model_predict_train__test_data(logreg_model, train_embedding_matrix, target_train,\n",
    "#                               test_embedding_matrix, target_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7541c437",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386f1ccf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a416634f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc7bfbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bdf5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('models/sgd_keras_model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
