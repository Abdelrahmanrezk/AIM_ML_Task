{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2595a258",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, LSTM, Embedding\n",
    "from configs import *\n",
    "from fetch_data import *\n",
    "from features_extraction import *\n",
    "from data_shuffling_split import *\n",
    "from data_preprocess import *\n",
    "from ml_modeling import *\n",
    "from keras_models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8deed7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of instances in the file are:  449033\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>dialect</th>\n",
       "      <th>dialect_l_encoded</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1056552188082716800</td>\n",
       "      <td>LY</td>\n",
       "      <td>8</td>\n",
       "      <td>توا دوشه الكلاسيكو شن بيتمها وشن بيسكتهم وشن ب...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>891734969202114560</td>\n",
       "      <td>SY</td>\n",
       "      <td>15</td>\n",
       "      <td>حسابشخصي في احلي من الشحاطه 😂</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1110565179257954432</td>\n",
       "      <td>SD</td>\n",
       "      <td>14</td>\n",
       "      <td>حسابشخصي موهبه والله 😂 اوع تحاول تطورها تقوم م...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1172817955270340608</td>\n",
       "      <td>LB</td>\n",
       "      <td>7</td>\n",
       "      <td>حسابشخصي حسابشخصي 😂 انا صرلي عشر سنين مش مجدده...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>293253217821790208</td>\n",
       "      <td>QA</td>\n",
       "      <td>12</td>\n",
       "      <td>احلي شعور تكون باجازه وتقوم من الصبح وتمر ع ال...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id dialect  dialect_l_encoded  \\\n",
       "0  1056552188082716800      LY                  8   \n",
       "1   891734969202114560      SY                 15   \n",
       "2  1110565179257954432      SD                 14   \n",
       "3  1172817955270340608      LB                  7   \n",
       "4   293253217821790208      QA                 12   \n",
       "\n",
       "                                                text  \n",
       "0  توا دوشه الكلاسيكو شن بيتمها وشن بيسكتهم وشن ب...  \n",
       "1                     حسابشخصي في احلي من الشحاطه 😂   \n",
       "2  حسابشخصي موهبه والله 😂 اوع تحاول تطورها تقوم م...  \n",
       "3  حسابشخصي حسابشخصي 😂 انا صرلي عشر سنين مش مجدده...  \n",
       "4  احلي شعور تكون باجازه وتقوم من الصبح وتمر ع ال...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strat_train_set = read_csv(\"train/strat_train_set.csv\")\n",
    "strat_train_set = strat_train_set.iloc[:5000]\n",
    "strat_train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df9dd098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of instances in the training data after StratifiedShuffleSplit are:  9800\n",
      "The number of instances in the testing data after StratifiedShuffleSplit are:   200\n",
      "The number of trainin instances:  9800\n",
      "The number of validation instances:  200\n",
      "The number of trainin labels :  9800\n",
      "The number of validation labels :  200\n"
     ]
    }
   ],
   "source": [
    "x_train_text, x_val_text, y_train, y_val = prepare_data(strat_train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0507dbfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Tokenization : \n",
      " ['حسابشخصي يا الله البنات عاطين سمعه عنا زباله 🤦 🏼 🤦 🏼 😭 احنا مش هيك والله 💔 ', 'حسابشخصي والله الزول ده كتر المحلبيه', 'حسابشخصي لين ادشين فيها تمي لعبي بالتلفون 🌚 ']\n",
      "==================================================\n",
      "After Tokenization : \n",
      " [['حسابشخصي', 'يا', 'الله', 'البنات', 'عاطين', 'سمعه', 'عنا', 'زباله', '🤦', '🏼', '🤦', '🏼', '😭', 'احنا', 'مش', 'هيك', 'والله', '💔'], ['حسابشخصي', 'والله', 'الزول', 'ده', 'كتر', 'المحلبيه'], ['حسابشخصي', 'لين', 'ادشين', 'فيها', 'تمي', 'لعبي', 'بالتلفون', '🌚']]\n",
      "==================================================\n",
      "Before Tokenization : \n",
      " ['لمحت زولك او جابوا بالحكي طاريك ،، #يرفرف_قلبي_اذا', 'حسابشخصي مهما عملت روسيا هتغطي علي الكل 😂 انا قصدي ماتشات المنتخب محدش يفهمني غلط مقولتش المشجعات الروسيات', '#طرابلس . الشعب الوحيد اللي تغم عليه نفسيته لما يشوفو حدوده']\n",
      "==================================================\n",
      "After Tokenization : \n",
      " [['لمحت', 'زولك', 'او', 'جابوا', 'بالحكي', 'طاريك', '،،', '#', 'يرفرف_قلبي_اذا'], ['حسابشخصي', 'مهما', 'عملت', 'روسيا', 'هتغطي', 'علي', 'الكل', '😂', 'انا', 'قصدي', 'ماتشات', 'المنتخب', 'محدش', 'يفهمني', 'غلط', 'مقولتش', 'المشجعات', 'الروسيات'], ['#', 'طرابلس', '.', 'الشعب', 'الوحيد', 'اللي', 'تغم', 'عليه', 'نفسيته', 'لما', 'يشوفو', 'حدوده']]\n"
     ]
    }
   ],
   "source": [
    "x_train_text_tokenized = tokenize_using_nltk_TreebankWordTokenizer(x_train_text)\n",
    "\n",
    "print(\"Before Tokenization : \\n\", x_train_text[:3])\n",
    "print(\"=\"*50)\n",
    "print(\"After Tokenization : \\n\", x_train_text_tokenized[:3])\n",
    "print(\"=\"*50)\n",
    "\n",
    "x_val_text_tokenized = tokenize_using_nltk_TreebankWordTokenizer(x_val_text)\n",
    "\n",
    "print(\"Before Tokenization : \\n\", x_val_text[:3])\n",
    "print(\"=\"*50)\n",
    "print(\"After Tokenization : \\n\", x_val_text_tokenized[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b40a9d",
   "metadata": {},
   "source": [
    "# LSTM Model with Bakr Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc85e489",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_vec_model = load_word2vec_model(\"models/word2vec/bakrianoo_unigram_cbow_model/full_uni_cbow_100_twitter.mdl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3316ece3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "(9800, 64, 100)\n",
      "(9800, 6400)\n",
      "==================================================\n",
      "[ 0.2341   1.27    -1.722   -7.742   -0.05646 -1.704    1.6      0.5825\n",
      " -0.4714  -4.09    -0.7295   0.6294   0.194    2.477   -1.136    2.854\n",
      " -0.6875   1.646    1.055   -9.82     0.2559  -0.919   -5.27    -1.97\n",
      "  1.414   -0.7124   1.286    1.83     1.561   -4.965   -2.19    -3.455\n",
      "  2.053    0.1661  -2.848   -2.291   -0.836    1.756    2.492   -1.278\n",
      " -1.596    1.31     3.03     2.994   -2.713    0.943    1.218    2.777\n",
      "  4.227   -0.533  ]\n",
      "==================================================\n",
      "(200, 64, 100)\n",
      "(200, 6400)\n",
      "==================================================\n",
      "[ 1.7686e+00 -6.7578e-01  6.1523e+00 -1.2939e+00 -2.3877e-01 -1.9150e+00\n",
      "  2.3379e+00  3.9160e+00  3.7266e+00  3.6172e+00  2.5801e+00 -6.0498e-01\n",
      " -1.4756e+00  2.8594e+00  1.3076e+00 -1.2031e+00 -9.7229e-02 -4.2065e-01\n",
      " -1.6821e-01 -8.4778e-02  6.3416e-02 -5.5029e-01  1.5127e+00 -6.1401e-02\n",
      " -8.5107e-01 -2.4688e+00 -2.7656e+00 -3.7695e+00  5.7297e-03  9.8682e-01\n",
      "  7.7305e+00 -1.6689e+00 -1.4238e+00  2.3633e+00  3.9038e-01  5.9424e-01\n",
      " -1.4375e+00  1.9690e-01 -2.0840e+00 -4.8086e+00 -2.0410e+00  4.8867e+00\n",
      " -3.6792e-01 -1.1432e-01  1.1533e+00 -4.5547e+00  1.0225e+00  3.7012e+00\n",
      "  6.0195e+00  1.9316e+00]\n"
     ]
    }
   ],
   "source": [
    "max_len_str = 64\n",
    "hid_num_neurons = 50\n",
    "learning_rate = .1\n",
    "epochs = 30\n",
    "\n",
    "performance_lr = keras.callbacks.ReduceLROnPlateau(factor=.5, patience=5)\n",
    "SGD_optimizer     =keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "Adam_optimizer = keras.optimizers.Adam(beta_1=0.9, beta_2=0.999)\n",
    "RMSprop_optimizer = keras.optimizers.RMSprop(learning_rate=learning_rate, rho=.9)\n",
    "\n",
    "\n",
    "callbacks_ = keras_callbacks(word2vec_type=\"bakr_word2vec\", model_type=\"lstm_no_batch\", learning_rate=learning_rate)\n",
    "callbacks_.append(performance_lr)\n",
    "\n",
    "number_of_features = 100\n",
    "word2vec_path = \"bakr/\"\n",
    "\n",
    "X_train_embed_matrix = text_to_matrix_using_word2vec(word_to_vec_model, x_train_text_tokenized, max_len_str)\n",
    "X_val_embed_matrix = text_to_matrix_using_word2vec(word_to_vec_model, x_val_text_tokenized, max_len_str)\n",
    "\n",
    "# Reshape because of deep learning model\n",
    "X_train_embed_matrix = X_train_embed_matrix.reshape(X_train_embed_matrix.shape[0], max_len_str, number_of_features)\n",
    "X_val_embed_matrix = X_val_embed_matrix.reshape(X_val_embed_matrix.shape[0], max_len_str, number_of_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487e9705",
   "metadata": {},
   "source": [
    "# With  SGD and No Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ecd86aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 64, 50)            30200     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64, 50)            0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 3200)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 18)                57618     \n",
      "=================================================================\n",
      "Total params: 87,818\n",
      "Trainable params: 87,818\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = lstm_no_batch_seqential_model_create(hid_num_neurons, max_len_str, number_of_features, dropout=.2)\n",
    "model = seqential_model_compile(model, SGD_optimizer)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6a99fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "307/307 [==============================] - 10s 28ms/step - loss: 2.4303 - accuracy: 0.2315 - val_loss: 2.1591 - val_accuracy: 0.3200\n",
      "Epoch 2/5\n",
      "307/307 [==============================] - 8s 26ms/step - loss: 2.0689 - accuracy: 0.3435 - val_loss: 2.0427 - val_accuracy: 0.3550\n",
      "Epoch 3/5\n",
      "307/307 [==============================] - 8s 26ms/step - loss: 1.8724 - accuracy: 0.4051 - val_loss: 2.0600 - val_accuracy: 0.3450\n",
      "Epoch 4/5\n",
      "307/307 [==============================] - 8s 25ms/step - loss: 1.7087 - accuracy: 0.4565 - val_loss: 2.0269 - val_accuracy: 0.3350\n",
      "Epoch 5/5\n",
      "307/307 [==============================] - 8s 26ms/step - loss: 1.5757 - accuracy: 0.5062 - val_loss: 2.1113 - val_accuracy: 0.3500\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_embed_matrix, y_train, batch_size=32, epochs=epochs, validation_data=(X_val_embed_matrix, y_val),\n",
    "                   callbacks=callbacks_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89de34b",
   "metadata": {},
   "source": [
    "# With  Adam and No Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd31ae34",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lstm_no_batch_seqential_model_create(hid_num_neurons, max_len_str, number_of_features, dropout=.2)\n",
    "model = seqential_model_compile(model, Adam_optimizer)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a3ddb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train_embed_matrix, y_train, batch_size=32, epochs=epochs, validation_data=(X_val_embed_matrix, y_val),\n",
    "                   callbacks=callbacks_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9778ad2b",
   "metadata": {},
   "source": [
    "# With  RMSprob and No Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fc81d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lstm_no_batch_seqential_model_create(hid_num_neurons, max_len_str, number_of_features, dropout=.2)\n",
    "model = seqential_model_compile(model, RMSprop_optimizer)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81271f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train_embed_matrix, y_train, batch_size=32, epochs=epochs, validation_data=(X_val_embed_matrix, y_val),\n",
    "                   callbacks=callbacks_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f5486d",
   "metadata": {},
   "source": [
    "# With  SGD and Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe885c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks_ = keras_callbacks(word2vec_type=\"bakr_word2vec\", model_type=\"lstm_with_batch\", learning_rate=learning_rate)\n",
    "callbacks_.append(performance_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e560845",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lstm_with_batch_model_create(hid_num_neurons, max_len_str, number_of_features, dropout=.2)\n",
    "model = seqential_model_compile(model, SGD_optimizer)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946ae891",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train_embed_matrix, y_train, batch_size=32, epochs=epochs, validation_data=(X_val_embed_matrix, y_val),\n",
    "                   callbacks=callbacks_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01e6b55",
   "metadata": {},
   "source": [
    "# With  Adam and No Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a63ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lstm_with_batch_model_create(hid_num_neurons, max_len_str, number_of_features, dropout=.2)\n",
    "model = seqential_model_compile(model, Adam_optimizer)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f891fb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train_embed_matrix, y_train, batch_size=32, epochs=epochs, validation_data=(X_val_embed_matrix, y_val),\n",
    "                   callbacks=callbacks_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f15d10",
   "metadata": {},
   "source": [
    "# With  RMSprob and No Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33ba220",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lstm_with_batch_model_create(hid_num_neurons, max_len_str, number_of_features, dropout=.2)\n",
    "model = seqential_model_compile(model, RMSprop_optimizer)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae74e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train_embed_matrix, y_train, batch_size=32, epochs=epochs, validation_data=(X_val_embed_matrix, y_val),\n",
    "                   callbacks=callbacks_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b373ffc1",
   "metadata": {},
   "source": [
    "# LSTM Model with Rezk Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33410151",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_vec_model = load_word2vec_model(\"models/word2vec/rezk_unigram_CBOW_model/train_word2vec_cbow__window_3_min_count_300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47215e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_features = 300\n",
    "\n",
    "callbacks_ = keras_callbacks(word2vec_type=\"rezk_word2vec\", model_type=\"lstm_no_batch\", learning_rate=learning_rate)\n",
    "callbacks_.append(performance_lr)\n",
    "\n",
    "word2vec_path = \"rezk/\"\n",
    "\n",
    "X_train_embed_matrix = text_to_matrix_using_word2vec(word_to_vec_model, x_train_text_tokenized, max_len_str)\n",
    "X_val_embed_matrix = text_to_matrix_using_word2vec(word_to_vec_model, x_val_text_tokenized, max_len_str)\n",
    "\n",
    "# Reshape because of deep learning model\n",
    "X_train_embed_matrix = X_train_embed_matrix.reshape(X_train_embed_matrix.shape[0], max_len_str, number_of_features)\n",
    "X_val_embed_matrix = X_val_embed_matrix.reshape(X_val_embed_matrix.shape[0], max_len_str, number_of_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2b5bc8",
   "metadata": {},
   "source": [
    "# With  SGD and No Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357395ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lstm_no_batch_seqential_model_create(hid_num_neurons, max_len_str, number_of_features, dropout=.2)\n",
    "model = seqential_model_compile(model, SGD_optimizer)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2a571e",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train_embed_matrix, y_train, batch_size=32, epochs=epochs, validation_data=(X_val_embed_matrix, y_val),\n",
    "                   callbacks=callbacks_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666571bd",
   "metadata": {},
   "source": [
    "# With  Adam and No Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3037be",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lstm_no_batch_seqential_model_create(hid_num_neurons, max_len_str, number_of_features, dropout=.2)\n",
    "model = seqential_model_compile(model, Adam_optimizer)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cb499f",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train_embed_matrix, y_train, batch_size=32, epochs=epochs, validation_data=(X_val_embed_matrix, y_val),\n",
    "                   callbacks=callbacks_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb80d80",
   "metadata": {},
   "source": [
    "# With  Rmsprob and No Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ead3e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lstm_no_batch_seqential_model_create(hid_num_neurons, max_len_str, number_of_features, dropout=.2)\n",
    "model = seqential_model_compile(model, RMSprop_optimizer)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d88e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train_embed_matrix, y_train, batch_size=32, epochs=epochs, validation_data=(X_val_embed_matrix, y_val),\n",
    "                   callbacks=callbacks_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1361d56d",
   "metadata": {},
   "source": [
    "# With  SGD and  Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e74a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks_ = keras_callbacks(word2vec_type=\"rezk_word2vec\", model_type=\"lstm_with_batch\", learning_rate=learning_rate)\n",
    "callbacks_.append(performance_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8465f850",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lstm_with_batch_model_create(hid_num_neurons, max_len_str, number_of_features, dropout=.2)\n",
    "model = seqential_model_compile(model, SGD_optimizer)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e838b51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train_embed_matrix, y_train, batch_size=32, epochs=epochs, validation_data=(X_val_embed_matrix, y_val),\n",
    "                   callbacks=callbacks_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ae6cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lstm_with_batch_model_create(hid_num_neurons, max_len_str, number_of_features, dropout=.2)\n",
    "model = seqential_model_compile(model, Adam_optimizer)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f05e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train_embed_matrix, y_train, batch_size=32, epochs=epochs, validation_data=(X_val_embed_matrix, y_val),\n",
    "                   callbacks=callbacks_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766bd32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lstm_with_batch_model_create(hid_num_neurons, max_len_str, number_of_features, dropout=.2)\n",
    "model = seqential_model_compile(model, RMSprop_optimizer)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7261098d",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train_embed_matrix, y_train, batch_size=32, epochs=epochs, validation_data=(X_val_embed_matrix, y_val),\n",
    "                   callbacks=callbacks_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
