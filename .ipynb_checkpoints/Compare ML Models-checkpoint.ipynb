{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2fa8549",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from data_shuffling_split import *\n",
    "from features_extraction import *\n",
    "from data_preprocess import *\n",
    "from ml_modeling import *\n",
    "from configs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f41f6338",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_score(model, x_train, y_train, x_val, y_val, x_test, y_test):\n",
    "    \n",
    "    print(\"On Training set\\n\")\n",
    "    f1_score_result(model, x_train, y_train)\n",
    "    print(\"=\"*50)\n",
    "    print(\"On Validation set \\n\")\n",
    "    f1_score_result(model, x_val, y_val)\n",
    "    print(\"=\"*50)\n",
    "    print(\"On Training \\n\")\n",
    "    f1_score_result(model, x_test, y_test)\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f96fb7",
   "metadata": {},
   "source": [
    "# Tokenize All data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ade6d3cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of instances in the file are:  449033\n",
      "The number of instances in the training data after StratifiedShuffleSplit are:  440052\n",
      "The number of instances in the testing data after StratifiedShuffleSplit are:   8981\n",
      "The number of trainin instances:  440052\n",
      "The number of validation instances:  8981\n",
      "The number of trainin labels :  440052\n",
      "The number of validation labels :  8981\n",
      "Before Tokenization : \n",
      " ['حسابشخصي وشن دخل اهالي طرابلس يا جاهل', 'رساله توعويه تحذيريه من شرطه ابوظبي للجميع بتوخي الحذر في ظل سوء الاحوال الجويه شكرا شرطه ابوظبي رابطويب', 'يسعدوو الدكتوور قلي انا حتكفل بالمعمل تتعبيش نفسك 😭 ❤❤']\n",
      "==================================================\n",
      "After Tokenization : \n",
      " [['حسابشخصي', 'وشن', 'دخل', 'اهالي', 'طرابلس', 'يا', 'جاهل'], ['رساله', 'توعويه', 'تحذيريه', 'من', 'شرطه', 'ابوظبي', 'للجميع', 'بتوخي', 'الحذر', 'في', 'ظل', 'سوء', 'الاحوال', 'الجويه', 'شكرا', 'شرطه', 'ابوظبي', 'رابطويب'], ['يسعدوو', 'الدكتوور', 'قلي', 'انا', 'حتكفل', 'بالمعمل', 'تتعبيش', 'نفسك', '😭', '❤❤']]\n",
      "==================================================\n",
      "Before Tokenization : \n",
      " ['حسابشخصي ااشر علي القمر والاخ يطالع اصبعي . . انا اقصد المباراه الاخيره . . سؤال من سيد اسيا ٢٠١٩ ؟ بس كل زق مابي اعرف 😆 ', 'حسابشخصي دكتورنا ايام الجامعه 😍 ما شاء الله عليه', 'حسابشخصي يمكن الشوط الثاني نشوفه']\n",
      "==================================================\n",
      "After Tokenization : \n",
      " [['حسابشخصي', 'ااشر', 'علي', 'القمر', 'والاخ', 'يطالع', 'اصبعي', '.', '.', 'انا', 'اقصد', 'المباراه', 'الاخيره', '.', '.', 'سؤال', 'من', 'سيد', 'اسيا', '٢٠١٩', '؟', 'بس', 'كل', 'زق', 'مابي', 'اعرف', '😆'], ['حسابشخصي', 'دكتورنا', 'ايام', 'الجامعه', '😍', 'ما', 'شاء', 'الله', 'عليه'], ['حسابشخصي', 'يمكن', 'الشوط', 'الثاني', 'نشوفه']]\n",
      "Before Tokenization : \n",
      " ['ياكثرهم في زمانك وياقلهم في وفاك : ياما سمعنا القصايد لكنها ما تفيد العذر والغدر واضح تجني طريق الهلاك : اما حفظت المواصل والا خسرت الرصيد', 'حسابشخصي انزين انت الحين ما عرفت التسعيره عشان تقول له go head شلون تبي السياره تستوي ؟ ', 'حسابشخصي ماعندكمش مطبخ في حوش فيه جو را 😂 ']\n",
      "==================================================\n",
      "After Tokenization : \n",
      " [['ياكثرهم', 'في', 'زمانك', 'وياقلهم', 'في', 'وفاك', ':', 'ياما', 'سمعنا', 'القصايد', 'لكنها', 'ما', 'تفيد', 'العذر', 'والغدر', 'واضح', 'تجني', 'طريق', 'الهلاك', ':', 'اما', 'حفظت', 'المواصل', 'والا', 'خسرت', 'الرصيد'], ['حسابشخصي', 'انزين', 'انت', 'الحين', 'ما', 'عرفت', 'التسعيره', 'عشان', 'تقول', 'له', 'go', 'head', 'شلون', 'تبي', 'السياره', 'تستوي', '؟'], ['حسابشخصي', 'ماعندكمش', 'مطبخ', 'في', 'حوش', 'فيه', 'جو', 'را', '😂']]\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Train and Validation data\n",
    "strat_train_set = read_csv(\"train/strat_train_set.csv\")\n",
    "x_train_text, x_val_text, y_train, y_val = prepare_data(strat_train_set)\n",
    "\n",
    "# Test\n",
    "strat_test_set = pd.read_csv(\"dataset/test/strat_test_set.csv\")\n",
    "x_test_text, y_test = list(strat_test_set['text']), strat_test_set['dialect_l_encoded'].values\n",
    "\n",
    "\n",
    "x_train_text_tokenized = tokenize_using_nltk_TreebankWordTokenizer(x_train_text)\n",
    "\n",
    "print(\"Before Tokenization : \\n\", x_train_text[:3])\n",
    "print(\"=\"*50)\n",
    "print(\"After Tokenization : \\n\", x_train_text_tokenized[:3])\n",
    "print(\"=\"*50)\n",
    "\n",
    "x_val_text_tokenized = tokenize_using_nltk_TreebankWordTokenizer(x_val_text)\n",
    "\n",
    "print(\"Before Tokenization : \\n\", x_val_text[:3])\n",
    "print(\"=\"*50)\n",
    "print(\"After Tokenization : \\n\", x_val_text_tokenized[:3])\n",
    "\n",
    "\n",
    "x_test_text_tokenized = tokenize_using_nltk_TreebankWordTokenizer(x_test_text)\n",
    "\n",
    "print(\"Before Tokenization : \\n\", x_test_text[:3])\n",
    "print(\"=\"*50)\n",
    "print(\"After Tokenization : \\n\", x_test_text_tokenized[:3])\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8352687a",
   "metadata": {},
   "source": [
    "# Abo Bakr Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbc36cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(440052, 6400)\n",
      "==================================================\n",
      "[-0.05905 -0.0757   0.4565  -0.6665  -0.597   -0.595    0.823   -1.165\n",
      "  0.8506   1.134   -0.2147  -1.182    1.329   -1.651   -2.314    1.583\n",
      " -0.129    0.3357   0.399   -0.605    1.994    1.381   -0.942    1.259\n",
      "  2.197   -0.19    -1.18    -0.93    -1.755   -0.8086   0.646   -0.9424\n",
      "  1.502    0.7266   1.449    1.867    0.3801   1.539   -0.05664  0.728\n",
      " -0.69    -0.884   -0.7812   0.823   -1.819    0.06198  0.145   -0.08875\n",
      "  1.484    0.3823 ]\n",
      "(8981, 6400)\n",
      "==================================================\n",
      "[ 0.3884   0.5273  -0.6177   0.6475  -1.007   -1.808    0.02069 -0.5015\n",
      "  0.6987   0.0622   0.2275  -0.10034 -0.9834   0.8125  -0.3647   0.1355\n",
      " -0.4746  -0.5894   0.1296   0.8384  -0.3652   0.8203  -1.062    0.7207\n",
      " -1.255   -1.192   -0.1323   0.03262  0.8306   0.2048   0.738    0.00873\n",
      " -0.3853   1.633   -0.4583  -0.8047  -1.7     -0.05905 -1.256    0.6904\n",
      " -0.4019  -0.7856   1.004    1.708    0.5195  -1.478    0.9395   0.4226\n",
      " -0.0227  -0.2229 ]\n",
      "(9164, 6400)\n",
      "==================================================\n",
      "[-0.2822  -0.1575   3.271   -5.48    -0.6846   1.536    0.1155   0.1368\n",
      " -2.66    -2.887    0.4595   0.6196  -0.5166   0.4082   0.3364  -1.882\n",
      " -2.713   -2.545    0.4863   0.1699   0.01108  1.388   -2.156    2.8\n",
      " -2.316    1.119   -0.357   -0.5176  -0.3162   0.2896   1.391   -0.1215\n",
      " -0.7866   1.626    2.725   -0.8965   0.9175  -1.507   -1.036   -0.629\n",
      " -0.4058   0.0749   1.287    2.66    -1.376   -1.145    2.055   -0.1971\n",
      " -0.327   -3.309  ]\n"
     ]
    }
   ],
   "source": [
    "number_of_features = 100\n",
    "max_len_str = 64\n",
    "\n",
    "word_to_vec_model = load_word2vec_model(\"models/word2vec/bakrianoo_unigram_cbow_model/full_uni_cbow_100_twitter.mdl\")\n",
    "\n",
    "X_train_embed_matrix = text_to_matrix_using_word2vec(word_to_vec_model, x_train_text_tokenized, max_len_str)\n",
    "X_val_embed_matrix = text_to_matrix_using_word2vec(word_to_vec_model, x_val_text_tokenized, max_len_str)\n",
    "x_test_embed_matrix = text_to_matrix_using_word2vec(word_to_vec_model, x_test_text_tokenized, max_len_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53b0d272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On Training set\n",
      "\n",
      "===================== Validate Result =====================\n",
      "F1 score is:  0.3367897430303691\n",
      "==================================================\n",
      "On Validation set \n",
      "\n",
      "===================== Validate Result =====================\n",
      "F1 score is:  0.3323683331477564\n",
      "==================================================\n",
      "On Training \n",
      "\n",
      "===================== Validate Result =====================\n",
      "F1 score is:  0.32802269751200347\n"
     ]
    }
   ],
   "source": [
    "# Test using AdaBoostClassifier \n",
    "\n",
    "model_path    = \"bakr/AdaBoostClassifier__f1_0.325_ml.sav\"\n",
    "model = pickle_load_model(\"models/ml_models/\" + model_path)\n",
    "\n",
    "_ = train_val_test_score(model, X_train_embed_matrix, y_train, X_val_embed_matrix, y_val, \n",
    "                         x_test_embed_matrix, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc55d829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On Training set\n",
      "\n",
      "===================== Validate Result =====================\n",
      "F1 score is:  0.4002072482342996\n",
      "==================================================\n",
      "On Validation set \n",
      "\n",
      "===================== Validate Result =====================\n",
      "F1 score is:  0.4007348847567086\n",
      "==================================================\n",
      "On Training \n",
      "\n",
      "===================== Validate Result =====================\n",
      "F1 score is:  0.3644696639022261\n"
     ]
    }
   ],
   "source": [
    "# Test using Logistic Regression \n",
    "model_path    = \"bakr/unigram_100d_lg_cls_model_f1_0.366.sav\"\n",
    "model = pickle_load_model(\"models/ml_models/\" + model_path)\n",
    "\n",
    "_ = train_val_test_score(model, X_train_embed_matrix, y_train, X_val_embed_matrix, y_val, \n",
    "                         x_test_embed_matrix, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a129b7",
   "metadata": {},
   "source": [
    "#  Rezk Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "165e7636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(440052, 19200)\n",
      "==================================================\n",
      "[-0.1262   0.2761   0.2466  -0.3464  -0.5044   0.216    0.2651   0.05423\n",
      " -0.3276  -0.2793   0.328    0.1699  -0.05267  0.1941   0.292    0.1654\n",
      " -0.01619 -0.428    0.411    0.0927   0.271    0.6206  -0.04764  0.04465\n",
      "  0.0863   0.06042  0.08374 -0.0927   0.05176 -0.1616  -0.4875   0.4932\n",
      "  0.1333   0.4666   0.0387  -0.19     0.05563 -0.1526   0.549    0.2966\n",
      " -0.0969  -0.345   -0.2896  -0.0667   0.12146  0.2126   0.1146  -0.4404\n",
      " -0.1198   0.2651 ]\n",
      "(8981, 19200)\n",
      "==================================================\n",
      "[-0.1262   0.2761   0.2466  -0.3464  -0.5044   0.216    0.2651   0.05423\n",
      " -0.3276  -0.2793   0.328    0.1699  -0.05267  0.1941   0.292    0.1654\n",
      " -0.01619 -0.428    0.411    0.0927   0.271    0.6206  -0.04764  0.04465\n",
      "  0.0863   0.06042  0.08374 -0.0927   0.05176 -0.1616  -0.4875   0.4932\n",
      "  0.1333   0.4666   0.0387  -0.19     0.05563 -0.1526   0.549    0.2966\n",
      " -0.0969  -0.345   -0.2896  -0.0667   0.12146  0.2126   0.1146  -0.4404\n",
      " -0.1198   0.2651 ]\n",
      "(9164, 19200)\n",
      "==================================================\n",
      "[ 0.629     1.157    -0.6753    0.2048    0.2107    0.516    -0.6455\n",
      "  0.669    -0.72      1.04      0.2937   -0.4473   -0.2487    0.1098\n",
      " -0.1501    0.625     0.012115  0.2568   -1.126     0.2261   -0.08954\n",
      "  0.4692    0.1472    0.4668   -0.0946    0.3938    0.1757    0.413\n",
      " -0.189     0.03577  -0.816     0.8457   -0.3623   -0.564     0.619\n",
      "  0.3655    0.714    -0.3547    0.0713    1.005     0.594     0.671\n",
      "  0.7793    0.1586    0.2812   -0.11224  -0.7334    0.2106    0.4324\n",
      "  0.941   ]\n"
     ]
    }
   ],
   "source": [
    "number_of_features = 100\n",
    "max_len_str = 64\n",
    "\n",
    "word2vec_path = \"rezk_unigram_CBOW_model/train_word2vec_cbow__window_3_min_count_300\"\n",
    "word_to_vec_model = load_word2vec_model(\"models/word2vec/\" + word2vec_path)\n",
    "\n",
    "X_train_embed_matrix = text_to_matrix_using_word2vec(word_to_vec_model, x_train_text_tokenized, max_len_str)\n",
    "X_val_embed_matrix = text_to_matrix_using_word2vec(word_to_vec_model, x_val_text_tokenized, max_len_str)\n",
    "x_test_embed_matrix = text_to_matrix_using_word2vec(word_to_vec_model, x_test_text_tokenized, max_len_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c244353a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On Training set\n",
      "\n",
      "===================== Validate Result =====================\n",
      "F1 score is:  0.5133302427894885\n",
      "==================================================\n",
      "On Validation set \n",
      "\n",
      "===================== Validate Result =====================\n",
      "F1 score is:  0.5129718294176595\n",
      "==================================================\n",
      "On Training \n",
      "\n",
      "===================== Validate Result =====================\n",
      "F1 score is:  0.4143387167175906\n"
     ]
    }
   ],
   "source": [
    "# Test using LogisticRegression \n",
    "\n",
    "model_path    = \"rezk/LogisticRegression__f1_0.41_ml.sav\"\n",
    "model = pickle_load_model(\"models/ml_models/\" + model_path)\n",
    "\n",
    "_ = train_val_test_score(model, X_train_embed_matrix, y_train, X_val_embed_matrix, y_val, \n",
    "                         x_test_embed_matrix, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d400b0",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "If we compare the model prediction to the human prediction, we may conclude that the tasks of predict the dialect is difficult task for human. how its compare to the model. maybe its simpler if we talked about voice recogniation, as new layers comes in the voice it self and how people speak, the phonems, and others, its not as the same as the text.\n",
    "\n",
    "And as in the reference paper the SVC model have over **50%**, but we need more resource to train and wait for the model.\n",
    "\n",
    "Also I would like to use AdaBoostClassifier at the end with the API, as if we compared to human prediction of the text, its the best ones as we have very small gap between training and validation, and its doing the same for testing data which never seeing by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abf95d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
