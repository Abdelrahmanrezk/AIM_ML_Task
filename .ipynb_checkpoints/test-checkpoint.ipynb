{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09c7d882",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, LSTM, Embedding\n",
    "from configs import *\n",
    "from fetch_data import *\n",
    "from features_extraction import *\n",
    "from data_shuffling_split import *\n",
    "from data_preprocess import *\n",
    "from ml_modeling import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d387e9a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of instances in the file are:  449033\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>dialect</th>\n",
       "      <th>dialect_l_encoded</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1056552188082716800</td>\n",
       "      <td>LY</td>\n",
       "      <td>8</td>\n",
       "      <td>ุชูุง ุฏูุดู ุงูููุงุณููู ุดู ุจูุชููุง ูุดู ุจูุณูุชูู ูุดู ุจ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>891734969202114560</td>\n",
       "      <td>SY</td>\n",
       "      <td>15</td>\n",
       "      <td>ุญุณุงุจุดุฎุตู ูู ุงุญูู ูู ุงูุดุญุงุทู ๐</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1110565179257954432</td>\n",
       "      <td>SD</td>\n",
       "      <td>14</td>\n",
       "      <td>ุญุณุงุจุดุฎุตู ูููุจู ูุงููู ๐ ุงูุน ุชุญุงูู ุชุทูุฑูุง ุชููู ู...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1172817955270340608</td>\n",
       "      <td>LB</td>\n",
       "      <td>7</td>\n",
       "      <td>ุญุณุงุจุดุฎุตู ุญุณุงุจุดุฎุตู ๐ ุงูุง ุตุฑูู ุนุดุฑ ุณููู ูุด ูุฌุฏุฏู...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>293253217821790208</td>\n",
       "      <td>QA</td>\n",
       "      <td>12</td>\n",
       "      <td>ุงุญูู ุดุนูุฑ ุชููู ุจุงุฌุงุฒู ูุชููู ูู ุงูุตุจุญ ูุชูุฑ ุน ุงู...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id dialect  dialect_l_encoded  \\\n",
       "0  1056552188082716800      LY                  8   \n",
       "1   891734969202114560      SY                 15   \n",
       "2  1110565179257954432      SD                 14   \n",
       "3  1172817955270340608      LB                  7   \n",
       "4   293253217821790208      QA                 12   \n",
       "\n",
       "                                                text  \n",
       "0  ุชูุง ุฏูุดู ุงูููุงุณููู ุดู ุจูุชููุง ูุดู ุจูุณูุชูู ูุดู ุจ...  \n",
       "1                     ุญุณุงุจุดุฎุตู ูู ุงุญูู ูู ุงูุดุญุงุทู ๐   \n",
       "2  ุญุณุงุจุดุฎุตู ูููุจู ูุงููู ๐ ุงูุน ุชุญุงูู ุชุทูุฑูุง ุชููู ู...  \n",
       "3  ุญุณุงุจุดุฎุตู ุญุณุงุจุดุฎุตู ๐ ุงูุง ุตุฑูู ุนุดุฑ ุณููู ูุด ูุฌุฏุฏู...  \n",
       "4  ุงุญูู ุดุนูุฑ ุชููู ุจุงุฌุงุฒู ูุชููู ูู ุงูุตุจุญ ูุชูุฑ ุน ุงู...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strat_train_set = read_csv(\"train/strat_train_set.csv\")\n",
    "strat_train_set = strat_train_set.iloc[:5000]\n",
    "strat_train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff8fedd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_vec_model = load_word2vec_model(\"models/word2vec/bakrianoo_unigram_cbow_100_twitter/full_uni_cbow_100_twitter.mdl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "291746a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of instances in the training data after StratifiedShuffleSplit are:  4900\n",
      "The number of instances in the testing data after StratifiedShuffleSplit are:   100\n",
      "The number of trainin instances:  4900\n",
      "The number of validation instances:  100\n",
      "The number of trainin labels :  4900\n",
      "The number of validation labels :  100\n"
     ]
    }
   ],
   "source": [
    "x_train_text, x_val_text, y_train, y_val = prepare_data(strat_train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ac21d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Tokenization : \n",
      " ['ุชููููู ูุงุฏ ุนุดุงู ุชููุจ ููู ุ ! ุฑุงุจุทููุจ', 'ุญุณุงุจุดุฎุตู ูุณููู ูุงูุง ุฏุงูุจ ุดูู ูุญููู ุนุงูุฒ ุงุนุฑู ุจุณ ุทุฑููู ููููู', 'โูุญูุฏ ุญูุงูู - ุฑุณูู ูู ุฎูุงูู - ูู ุงูุจูู ูู ููู ูู ุฏู 2019 โ on # SoundCloud # np ุฑุงุจุทููุจ']\n",
      "==================================================\n",
      "After Tokenization : \n",
      " [['ุชููููู', 'ูุงุฏ', 'ุนุดุงู', 'ุชููุจ', 'ููู', 'ุ', '!', 'ุฑุงุจุทููุจ'], ['ุญุณุงุจุดุฎุตู', 'ูุณููู', 'ูุงูุง', 'ุฏุงูุจ', 'ุดูู', 'ูุญููู', 'ุนุงูุฒ', 'ุงุนุฑู', 'ุจุณ', 'ุทุฑููู', 'ููููู'], ['โูุญูุฏ', 'ุญูุงูู', '-', 'ุฑุณูู', 'ูู', 'ุฎูุงูู', '-', 'ูู', 'ุงูุจูู', 'ูู', 'ููู', 'ูู', 'ุฏู', '2019', 'โ', 'on', '#', 'SoundCloud', '#', 'np', 'ุฑุงุจุทููุจ']]\n",
      "==================================================\n",
      "Before Tokenization : \n",
      " ['ุจูุนุฌุจูู ุงููู ุจูุนูุฏ ุงุฎุชุฑุงุน ุงูุนุฌููุ ูุฌู ููููู ุนูู ููุฑู ูู ุทุฑู ุจูุฏูุฑ ุนูู ูุตูุญุชู ูู ุงูุณูุงุณู ! ุ ูุง ูุงูุงุฏ ูุงูุจููุ ูุง ุฎูุงุตู ุฒูุงูู ! ! ุ ูุง ุนุตุงุฑู ุฐูุงุก ุฌููุงุช ุนููุชู ! ! . . ูุงุฌุฆุชูู ูุนุงูุด ูู ููู ุงูุตุฏููุ ุงููุงุณ ุงููุญุดู ุจุชุฏูุฑ ุนูู ูุตูุญุชูุง ูู ุงูุณูุงุณู', 'ุญุณุงุจุดุฎุตู ูุงุฑูุช ูุชุญูู ูุฐุง ุงููุทูุจ ููู ุญุฏ ูุงุฎุฐ ุญูู ููุตุฑู ููู', 'ุฎุณุงุฑู ููู ุงู ูููู ุณูุนุงูุง ููู ูุง ููุงูู ุจุชุฌุฑู ูุฑุง ุงููููุณ ุฎุจูุซ ุฑุงุจุทููุจ']\n",
      "==================================================\n",
      "After Tokenization : \n",
      " [['ุจูุนุฌุจูู', 'ุงููู', 'ุจูุนูุฏ', 'ุงุฎุชุฑุงุน', 'ุงูุนุฌููุ', 'ูุฌู', 'ููููู', 'ุนูู', 'ููุฑู', 'ูู', 'ุทุฑู', 'ุจูุฏูุฑ', 'ุนูู', 'ูุตูุญุชู', 'ูู', 'ุงูุณูุงุณู', '!', 'ุ', 'ูุง', 'ูุงูุงุฏ', 'ูุงูุจููุ', 'ูุง', 'ุฎูุงุตู', 'ุฒูุงูู', '!', '!', 'ุ', 'ูุง', 'ุนุตุงุฑู', 'ุฐูุงุก', 'ุฌููุงุช', 'ุนููุชู', '!', '!', '.', '.', 'ูุงุฌุฆุชูู', 'ูุนุงูุด', 'ูู', 'ููู', 'ุงูุตุฏููุ', 'ุงููุงุณ', 'ุงููุญุดู', 'ุจุชุฏูุฑ', 'ุนูู', 'ูุตูุญุชูุง', 'ูู', 'ุงูุณูุงุณู'], ['ุญุณุงุจุดุฎุตู', 'ูุงุฑูุช', 'ูุชุญูู', 'ูุฐุง', 'ุงููุทูุจ', 'ููู', 'ุญุฏ', 'ูุงุฎุฐ', 'ุญูู', 'ููุตุฑู', 'ููู'], ['ุฎุณุงุฑู', 'ููู', 'ุงู', 'ูููู', 'ุณูุนุงูุง', 'ููู', 'ูุง', 'ููุงูู', 'ุจุชุฌุฑู', 'ูุฑุง', 'ุงููููุณ', 'ุฎุจูุซ', 'ุฑุงุจุทููุจ']]\n"
     ]
    }
   ],
   "source": [
    "x_train_text_tokenized = tokenize_using_nltk_TreebankWordTokenizer(x_train_text)\n",
    "\n",
    "print(\"Before Tokenization : \\n\", x_train_text[:3])\n",
    "print(\"=\"*50)\n",
    "print(\"After Tokenization : \\n\", x_train_text_tokenized[:3])\n",
    "print(\"=\"*50)\n",
    "\n",
    "x_val_text_tokenized = tokenize_using_nltk_TreebankWordTokenizer(x_val_text)\n",
    "\n",
    "print(\"Before Tokenization : \\n\", x_val_text[:3])\n",
    "print(\"=\"*50)\n",
    "print(\"After Tokenization : \\n\", x_val_text_tokenized[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c24d3ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4900, 6400)\n",
      "==================================================\n",
      "[-0.802   2.176  -2.914  -1.148   2.05   -2.33    1.976   1.35   -2.018\n",
      "  3.11   -4.49   -1.446  -0.4963  1.45    3.727   6.41    0.308   2.55\n",
      " -1.509   1.952  -1.707   0.3433 -1.031   3.2    -0.786   0.662   1.399\n",
      "  4.195  -2.088   2.129  -3.22   -0.838   0.628   1.369  -1.197  -1.782\n",
      "  0.4106 -0.4167 -4.215  -3.604  -3.838   1.367   4.188   0.0954  3.195\n",
      "  0.9663  2.469  -1.191  -2.746   2.016 ]\n",
      "(100, 6400)\n",
      "==================================================\n",
      "[ 1.6875e+00 -6.6943e-01  8.7451e-01 -5.8936e-01  2.1484e+00  2.8594e+00\n",
      "  6.5332e-01 -4.9512e-01  2.9395e+00  1.2568e+00  1.4512e+00  1.1553e+00\n",
      "  3.0801e+00 -6.3916e-01 -3.1914e+00 -1.0459e+00 -3.2363e+00  6.2744e-01\n",
      "  1.1836e+00 -1.2139e+00  2.3535e+00 -2.0098e+00 -9.1064e-01 -9.7803e-01\n",
      " -9.3701e-01  1.5361e+00  1.4443e+00  1.4902e+00  5.0586e-01 -3.6074e+00\n",
      "  1.0488e+00  2.2520e+00 -1.6611e+00 -1.4854e+00  1.4746e+00 -1.5986e+00\n",
      "  2.6512e-03  3.1074e+00 -1.1035e+00 -4.5703e-01 -4.8096e-01  3.5000e+00\n",
      " -1.5557e+00 -4.2109e+00 -7.6611e-01 -3.0664e+00  4.7424e-02 -1.6221e+00\n",
      " -5.2197e-01  5.4248e-01]\n"
     ]
    }
   ],
   "source": [
    "number_of_features = 100\n",
    "max_len_str = 64\n",
    "word2vec_path = \"rezk/\"\n",
    "model_path_to_save = \"models/ml_models/\"\n",
    "estimators = voting_models()\n",
    "\n",
    "X_train_embed_matrix = text_to_matrix_using_word2vec(word_to_vec_model, x_train_text_tokenized, max_len_str)\n",
    "X_val_embed_matrix = text_to_matrix_using_word2vec(word_to_vec_model, x_val_text_tokenized, max_len_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dabf4652",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4900, 6400)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_embed_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2392243b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_embed_matrix = X_train_embed_matrix.reshape([X_train_embed_matrix.shape[0], max_len_str, number_of_features])\n",
    "X_val_embed_matrix  = X_val_embed_matrix.reshape([X_val_embed_matrix.shape[0], max_len_str, number_of_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "383cd7ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4900, 64, 100)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_embed_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa25b8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_vectorize(train_text, word_to_vec_model):\n",
    "    tokenizer = TreebankWordTokenizer()\n",
    "    vectorize_data = []\n",
    "    for sampel in train_text:\n",
    "        tokens = tokenizer.tokenize(sampel)\n",
    "        sampel_vec=[]\n",
    "        for token in tokens:\n",
    "            try:\n",
    "                sampel_vec.append(word_to_vec_model.wv[token])\n",
    "            except KeyError:\n",
    "                pass\n",
    "        vectorize_data.append(sampel_vec)\n",
    "    return vectorize_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d6fe0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tokenize_and_vectorize(x_train_text, word_to_vec_model)\n",
    "X_val = tokenize_and_vectorize(x_val_text, word_to_vec_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49477d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed967602",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06144d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_trunc(data, maxlen):\n",
    "    new_data = []\n",
    "    zero_vector = []\n",
    "    for _ in range(len(data[0][0])):\n",
    "        zero_vector.append(0.0)\n",
    "        \n",
    "    for sample in data:\n",
    "        if len(sample) > maxlen:\n",
    "            temp =sample[:maxlen]\n",
    "        elif len(sample) < maxlen:\n",
    "            temp=sample\n",
    "            additional_elems = maxlen - len(sample)\n",
    "            for _ in range(additional_elems):\n",
    "                temp.append(zero_vector)\n",
    "        else:\n",
    "            temp = sample\n",
    "        new_data.append(temp)\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d964ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pad_trunc(X_train, 64)\n",
    "X_val = pad_trunc(X_val, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b0d555de",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2f2b2d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nuros = 50\n",
    "model.add(LSTM(num_nuros, return_sequences=True, input_shape=(64, 100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0343a235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 64, 50)            30200     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64, 50)            0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 3200)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 18)                57618     \n",
      "=================================================================\n",
      "Total params: 87,818\n",
      "Trainable params: 87,818\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.add(Dropout(.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(18, activation=\"softmax\"))\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "         optimizer=\"sgd\",\n",
    "         metrics=\"accuracy\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfafe987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 50/154 [========>.....................] - ETA: 2s - loss: 2.8692 - accuracy: 0.0862"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_embed_matrix, y_train, batch_size=32, epochs=10, validation_data=(X_val_embed_matrix, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0b9fb2c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 64, 100)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val_embed_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddca33ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
