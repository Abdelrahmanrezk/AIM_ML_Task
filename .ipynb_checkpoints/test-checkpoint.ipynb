{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09c7d882",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, LSTM, Embedding\n",
    "from configs import *\n",
    "from fetch_data import *\n",
    "from features_extraction import *\n",
    "from data_shuffling_split import *\n",
    "from data_preprocess import *\n",
    "from ml_modeling import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d387e9a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of instances in the file are:  449033\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>dialect</th>\n",
       "      <th>dialect_l_encoded</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1056552188082716800</td>\n",
       "      <td>LY</td>\n",
       "      <td>8</td>\n",
       "      <td>ØªÙˆØ§ Ø¯ÙˆØ´Ù‡ Ø§Ù„ÙƒÙ„Ø§Ø³ÙŠÙƒÙˆ Ø´Ù† Ø¨ÙŠØªÙ…Ù‡Ø§ ÙˆØ´Ù† Ø¨ÙŠØ³ÙƒØªÙ‡Ù… ÙˆØ´Ù† Ø¨...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>891734969202114560</td>\n",
       "      <td>SY</td>\n",
       "      <td>15</td>\n",
       "      <td>Ø­Ø³Ø§Ø¨Ø´Ø®ØµÙŠ ÙÙŠ Ø§Ø­Ù„ÙŠ Ù…Ù† Ø§Ù„Ø´Ø­Ø§Ø·Ù‡ ğŸ˜‚</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1110565179257954432</td>\n",
       "      <td>SD</td>\n",
       "      <td>14</td>\n",
       "      <td>Ø­Ø³Ø§Ø¨Ø´Ø®ØµÙŠ Ù…ÙˆÙ‡Ø¨Ù‡ ÙˆØ§Ù„Ù„Ù‡ ğŸ˜‚ Ø§ÙˆØ¹ ØªØ­Ø§ÙˆÙ„ ØªØ·ÙˆØ±Ù‡Ø§ ØªÙ‚ÙˆÙ… Ù…...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1172817955270340608</td>\n",
       "      <td>LB</td>\n",
       "      <td>7</td>\n",
       "      <td>Ø­Ø³Ø§Ø¨Ø´Ø®ØµÙŠ Ø­Ø³Ø§Ø¨Ø´Ø®ØµÙŠ ğŸ˜‚ Ø§Ù†Ø§ ØµØ±Ù„ÙŠ Ø¹Ø´Ø± Ø³Ù†ÙŠÙ† Ù…Ø´ Ù…Ø¬Ø¯Ø¯Ù‡...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>293253217821790208</td>\n",
       "      <td>QA</td>\n",
       "      <td>12</td>\n",
       "      <td>Ø§Ø­Ù„ÙŠ Ø´Ø¹ÙˆØ± ØªÙƒÙˆÙ† Ø¨Ø§Ø¬Ø§Ø²Ù‡ ÙˆØªÙ‚ÙˆÙ… Ù…Ù† Ø§Ù„ØµØ¨Ø­ ÙˆØªÙ…Ø± Ø¹ Ø§Ù„...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id dialect  dialect_l_encoded  \\\n",
       "0  1056552188082716800      LY                  8   \n",
       "1   891734969202114560      SY                 15   \n",
       "2  1110565179257954432      SD                 14   \n",
       "3  1172817955270340608      LB                  7   \n",
       "4   293253217821790208      QA                 12   \n",
       "\n",
       "                                                text  \n",
       "0  ØªÙˆØ§ Ø¯ÙˆØ´Ù‡ Ø§Ù„ÙƒÙ„Ø§Ø³ÙŠÙƒÙˆ Ø´Ù† Ø¨ÙŠØªÙ…Ù‡Ø§ ÙˆØ´Ù† Ø¨ÙŠØ³ÙƒØªÙ‡Ù… ÙˆØ´Ù† Ø¨...  \n",
       "1                     Ø­Ø³Ø§Ø¨Ø´Ø®ØµÙŠ ÙÙŠ Ø§Ø­Ù„ÙŠ Ù…Ù† Ø§Ù„Ø´Ø­Ø§Ø·Ù‡ ğŸ˜‚   \n",
       "2  Ø­Ø³Ø§Ø¨Ø´Ø®ØµÙŠ Ù…ÙˆÙ‡Ø¨Ù‡ ÙˆØ§Ù„Ù„Ù‡ ğŸ˜‚ Ø§ÙˆØ¹ ØªØ­Ø§ÙˆÙ„ ØªØ·ÙˆØ±Ù‡Ø§ ØªÙ‚ÙˆÙ… Ù…...  \n",
       "3  Ø­Ø³Ø§Ø¨Ø´Ø®ØµÙŠ Ø­Ø³Ø§Ø¨Ø´Ø®ØµÙŠ ğŸ˜‚ Ø§Ù†Ø§ ØµØ±Ù„ÙŠ Ø¹Ø´Ø± Ø³Ù†ÙŠÙ† Ù…Ø´ Ù…Ø¬Ø¯Ø¯Ù‡...  \n",
       "4  Ø§Ø­Ù„ÙŠ Ø´Ø¹ÙˆØ± ØªÙƒÙˆÙ† Ø¨Ø§Ø¬Ø§Ø²Ù‡ ÙˆØªÙ‚ÙˆÙ… Ù…Ù† Ø§Ù„ØµØ¨Ø­ ÙˆØªÙ…Ø± Ø¹ Ø§Ù„...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strat_train_set = read_csv(\"train/strat_train_set.csv\")\n",
    "strat_train_set = strat_train_set.iloc[:5000]\n",
    "strat_train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff8fedd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_vec_model = load_word2vec_model(\"models/word2vec/bakrianoo_unigram_cbow_100_twitter/full_uni_cbow_100_twitter.mdl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "291746a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of instances in the training data after StratifiedShuffleSplit are:  4900\n",
      "The number of instances in the testing data after StratifiedShuffleSplit are:   100\n",
      "The number of trainin instances:  4900\n",
      "The number of validation instances:  100\n",
      "The number of trainin labels :  4900\n",
      "The number of validation labels :  100\n"
     ]
    }
   ],
   "source": [
    "x_train_text, x_val_text, y_train, y_val = prepare_data(strat_train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ac21d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Tokenization : \n",
      " ['ØªÙ„ÙÙˆÙ†Ùƒ Ù‡Ø§Ø¯ Ø¹Ø´Ø§Ù† ØªÙ‚Ù„Ø¨ ÙÙŠÙ‡ ØŸ ! Ø±Ø§Ø¨Ø·ÙˆÙŠØ¨', 'Ø­Ø³Ø§Ø¨Ø´Ø®ØµÙŠ ÙˆØ³Ù†ÙŠÙ† ÙˆØ§Ù†Ø§ Ø¯Ø§ÙŠØ¨ Ø´ÙˆÙ‚ ÙˆØ­Ù†ÙŠÙ† Ø¹Ø§ÙˆØ² Ø§Ø¹Ø±Ù Ø¨Ø³ Ø·Ø±ÙŠÙ‚Ùˆ Ù…Ù†ÙŠÙŠÙ†', 'â€˜Ù…Ø­Ù…Ø¯ Ø­Ù…Ø§Ù‚ÙŠ - Ø±Ø³Ù…Ùƒ ÙÙŠ Ø®ÙŠØ§Ù„ÙŠ - Ù…Ù† Ø§Ù„Ø¨ÙˆÙ… ÙƒÙ„ ÙŠÙˆÙ… Ù…Ù† Ø¯Ù‡ 2019 â€™ on # SoundCloud # np Ø±Ø§Ø¨Ø·ÙˆÙŠØ¨']\n",
      "==================================================\n",
      "After Tokenization : \n",
      " [['ØªÙ„ÙÙˆÙ†Ùƒ', 'Ù‡Ø§Ø¯', 'Ø¹Ø´Ø§Ù†', 'ØªÙ‚Ù„Ø¨', 'ÙÙŠÙ‡', 'ØŸ', '!', 'Ø±Ø§Ø¨Ø·ÙˆÙŠØ¨'], ['Ø­Ø³Ø§Ø¨Ø´Ø®ØµÙŠ', 'ÙˆØ³Ù†ÙŠÙ†', 'ÙˆØ§Ù†Ø§', 'Ø¯Ø§ÙŠØ¨', 'Ø´ÙˆÙ‚', 'ÙˆØ­Ù†ÙŠÙ†', 'Ø¹Ø§ÙˆØ²', 'Ø§Ø¹Ø±Ù', 'Ø¨Ø³', 'Ø·Ø±ÙŠÙ‚Ùˆ', 'Ù…Ù†ÙŠÙŠÙ†'], ['â€˜Ù…Ø­Ù…Ø¯', 'Ø­Ù…Ø§Ù‚ÙŠ', '-', 'Ø±Ø³Ù…Ùƒ', 'ÙÙŠ', 'Ø®ÙŠØ§Ù„ÙŠ', '-', 'Ù…Ù†', 'Ø§Ù„Ø¨ÙˆÙ…', 'ÙƒÙ„', 'ÙŠÙˆÙ…', 'Ù…Ù†', 'Ø¯Ù‡', '2019', 'â€™', 'on', '#', 'SoundCloud', '#', 'np', 'Ø±Ø§Ø¨Ø·ÙˆÙŠØ¨']]\n",
      "==================================================\n",
      "Before Tokenization : \n",
      " ['Ø¨ÙŠØ¹Ø¬Ø¨Ù†ÙŠ Ø§Ù„Ù„ÙŠ Ø¨ÙŠØ¹ÙŠØ¯ Ø§Ø®ØªØ±Ø§Ø¹ Ø§Ù„Ø¹Ø¬Ù„Ù‡ØŒ ÙŠØ¬ÙŠ ÙŠÙ‚ÙˆÙ„Ùƒ Ø¹Ù„ÙŠ ÙÙƒØ±Ù‡ ÙƒÙ„ Ø·Ø±Ù Ø¨ÙŠØ¯ÙˆØ± Ø¹Ù„ÙŠ Ù…ØµÙ„Ø­ØªÙ‡ ÙÙŠ Ø§Ù„Ø³ÙŠØ§Ø³Ù‡ ! ØŒ Ù„Ø§ ÙŠØ§ÙˆØ§Ø¯ ÙŠØ§Ù†Ø¨ÙŠÙ‡ØŒ ÙŠØ§ Ø®Ù„Ø§ØµÙ‡ Ø²Ù…Ø§Ù†Ùƒ ! ! ØŒ ÙŠØ§ Ø¹ØµØ§Ø±Ù‡ Ø°ÙƒØ§Ø¡ Ø¬ÙŠÙ†Ø§Øª Ø¹ÙŠÙ„ØªÙƒ ! ! . . ÙØ§Ø¬Ø¦ØªÙ†ÙŠ ÙˆØ¹Ø§ÙŠØ´ ÙÙŠ Ù‡ÙˆÙ„ Ø§Ù„ØµØ¯Ù…Ù‡ØŒ Ø§Ù„Ù†Ø§Ø³ Ø§Ù„ÙˆØ­Ø´Ù‡ Ø¨ØªØ¯ÙˆØ± Ø¹Ù„ÙŠ Ù…ØµÙ„Ø­ØªÙ‡Ø§ ÙÙŠ Ø§Ù„Ø³ÙŠØ§Ø³Ù‡', 'Ø­Ø³Ø§Ø¨Ø´Ø®ØµÙŠ ÙŠØ§Ø±ÙŠØª ÙŠØªØ­Ù‚Ù‚ Ù‡Ø°Ø§ Ø§Ù„Ù…Ø·Ù„Ø¨ ÙˆÙƒÙ„ Ø­Ø¯ ÙŠØ§Ø®Ø° Ø­Ù‚Ù‡ ÙˆÙŠØµØ±Ù ÙÙŠÙ‡', 'Ø®Ø³Ø§Ø±Ù‡ ÙÙŠÙƒ Ø§ÙŠ ÙƒÙ„Ù…Ù‡ Ø³Ù…Ø¹Ø§Ù†Ø§ Ù…Ù†Ùƒ ÙŠØ§ Ù…Ù†Ø§ÙÙ‚ Ø¨ØªØ¬Ø±ÙŠ ÙˆØ±Ø§ Ø§Ù„ÙÙ„ÙˆØ³ Ø®Ø¨ÙŠØ« Ø±Ø§Ø¨Ø·ÙˆÙŠØ¨']\n",
      "==================================================\n",
      "After Tokenization : \n",
      " [['Ø¨ÙŠØ¹Ø¬Ø¨Ù†ÙŠ', 'Ø§Ù„Ù„ÙŠ', 'Ø¨ÙŠØ¹ÙŠØ¯', 'Ø§Ø®ØªØ±Ø§Ø¹', 'Ø§Ù„Ø¹Ø¬Ù„Ù‡ØŒ', 'ÙŠØ¬ÙŠ', 'ÙŠÙ‚ÙˆÙ„Ùƒ', 'Ø¹Ù„ÙŠ', 'ÙÙƒØ±Ù‡', 'ÙƒÙ„', 'Ø·Ø±Ù', 'Ø¨ÙŠØ¯ÙˆØ±', 'Ø¹Ù„ÙŠ', 'Ù…ØµÙ„Ø­ØªÙ‡', 'ÙÙŠ', 'Ø§Ù„Ø³ÙŠØ§Ø³Ù‡', '!', 'ØŒ', 'Ù„Ø§', 'ÙŠØ§ÙˆØ§Ø¯', 'ÙŠØ§Ù†Ø¨ÙŠÙ‡ØŒ', 'ÙŠØ§', 'Ø®Ù„Ø§ØµÙ‡', 'Ø²Ù…Ø§Ù†Ùƒ', '!', '!', 'ØŒ', 'ÙŠØ§', 'Ø¹ØµØ§Ø±Ù‡', 'Ø°ÙƒØ§Ø¡', 'Ø¬ÙŠÙ†Ø§Øª', 'Ø¹ÙŠÙ„ØªÙƒ', '!', '!', '.', '.', 'ÙØ§Ø¬Ø¦ØªÙ†ÙŠ', 'ÙˆØ¹Ø§ÙŠØ´', 'ÙÙŠ', 'Ù‡ÙˆÙ„', 'Ø§Ù„ØµØ¯Ù…Ù‡ØŒ', 'Ø§Ù„Ù†Ø§Ø³', 'Ø§Ù„ÙˆØ­Ø´Ù‡', 'Ø¨ØªØ¯ÙˆØ±', 'Ø¹Ù„ÙŠ', 'Ù…ØµÙ„Ø­ØªÙ‡Ø§', 'ÙÙŠ', 'Ø§Ù„Ø³ÙŠØ§Ø³Ù‡'], ['Ø­Ø³Ø§Ø¨Ø´Ø®ØµÙŠ', 'ÙŠØ§Ø±ÙŠØª', 'ÙŠØªØ­Ù‚Ù‚', 'Ù‡Ø°Ø§', 'Ø§Ù„Ù…Ø·Ù„Ø¨', 'ÙˆÙƒÙ„', 'Ø­Ø¯', 'ÙŠØ§Ø®Ø°', 'Ø­Ù‚Ù‡', 'ÙˆÙŠØµØ±Ù', 'ÙÙŠÙ‡'], ['Ø®Ø³Ø§Ø±Ù‡', 'ÙÙŠÙƒ', 'Ø§ÙŠ', 'ÙƒÙ„Ù…Ù‡', 'Ø³Ù…Ø¹Ø§Ù†Ø§', 'Ù…Ù†Ùƒ', 'ÙŠØ§', 'Ù…Ù†Ø§ÙÙ‚', 'Ø¨ØªØ¬Ø±ÙŠ', 'ÙˆØ±Ø§', 'Ø§Ù„ÙÙ„ÙˆØ³', 'Ø®Ø¨ÙŠØ«', 'Ø±Ø§Ø¨Ø·ÙˆÙŠØ¨']]\n"
     ]
    }
   ],
   "source": [
    "x_train_text_tokenized = tokenize_using_nltk_TreebankWordTokenizer(x_train_text)\n",
    "\n",
    "print(\"Before Tokenization : \\n\", x_train_text[:3])\n",
    "print(\"=\"*50)\n",
    "print(\"After Tokenization : \\n\", x_train_text_tokenized[:3])\n",
    "print(\"=\"*50)\n",
    "\n",
    "x_val_text_tokenized = tokenize_using_nltk_TreebankWordTokenizer(x_val_text)\n",
    "\n",
    "print(\"Before Tokenization : \\n\", x_val_text[:3])\n",
    "print(\"=\"*50)\n",
    "print(\"After Tokenization : \\n\", x_val_text_tokenized[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c24d3ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4900, 6400)\n",
      "==================================================\n",
      "[-0.802   2.176  -2.914  -1.148   2.05   -2.33    1.976   1.35   -2.018\n",
      "  3.11   -4.49   -1.446  -0.4963  1.45    3.727   6.41    0.308   2.55\n",
      " -1.509   1.952  -1.707   0.3433 -1.031   3.2    -0.786   0.662   1.399\n",
      "  4.195  -2.088   2.129  -3.22   -0.838   0.628   1.369  -1.197  -1.782\n",
      "  0.4106 -0.4167 -4.215  -3.604  -3.838   1.367   4.188   0.0954  3.195\n",
      "  0.9663  2.469  -1.191  -2.746   2.016 ]\n",
      "(100, 6400)\n",
      "==================================================\n",
      "[ 1.6875e+00 -6.6943e-01  8.7451e-01 -5.8936e-01  2.1484e+00  2.8594e+00\n",
      "  6.5332e-01 -4.9512e-01  2.9395e+00  1.2568e+00  1.4512e+00  1.1553e+00\n",
      "  3.0801e+00 -6.3916e-01 -3.1914e+00 -1.0459e+00 -3.2363e+00  6.2744e-01\n",
      "  1.1836e+00 -1.2139e+00  2.3535e+00 -2.0098e+00 -9.1064e-01 -9.7803e-01\n",
      " -9.3701e-01  1.5361e+00  1.4443e+00  1.4902e+00  5.0586e-01 -3.6074e+00\n",
      "  1.0488e+00  2.2520e+00 -1.6611e+00 -1.4854e+00  1.4746e+00 -1.5986e+00\n",
      "  2.6512e-03  3.1074e+00 -1.1035e+00 -4.5703e-01 -4.8096e-01  3.5000e+00\n",
      " -1.5557e+00 -4.2109e+00 -7.6611e-01 -3.0664e+00  4.7424e-02 -1.6221e+00\n",
      " -5.2197e-01  5.4248e-01]\n"
     ]
    }
   ],
   "source": [
    "number_of_features = 100\n",
    "max_len_str = 64\n",
    "word2vec_path = \"rezk/\"\n",
    "model_path_to_save = \"models/ml_models/\"\n",
    "estimators = voting_models()\n",
    "\n",
    "X_train_embed_matrix = text_to_matrix_using_word2vec(word_to_vec_model, x_train_text_tokenized, max_len_str)\n",
    "X_val_embed_matrix = text_to_matrix_using_word2vec(word_to_vec_model, x_val_text_tokenized, max_len_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dabf4652",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4900, 6400)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_embed_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2392243b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_embed_matrix = X_train_embed_matrix.reshape([X_train_embed_matrix.shape[0], max_len_str, number_of_features])\n",
    "X_val_embed_matrix  = X_val_embed_matrix.reshape([X_val_embed_matrix.shape[0], max_len_str, number_of_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "383cd7ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4900, 64, 100)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_embed_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa25b8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_vectorize(train_text, word_to_vec_model):\n",
    "    tokenizer = TreebankWordTokenizer()\n",
    "    vectorize_data = []\n",
    "    for sampel in train_text:\n",
    "        tokens = tokenizer.tokenize(sampel)\n",
    "        sampel_vec=[]\n",
    "        for token in tokens:\n",
    "            try:\n",
    "                sampel_vec.append(word_to_vec_model.wv[token])\n",
    "            except KeyError:\n",
    "                pass\n",
    "        vectorize_data.append(sampel_vec)\n",
    "    return vectorize_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d6fe0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tokenize_and_vectorize(x_train_text, word_to_vec_model)\n",
    "X_val = tokenize_and_vectorize(x_val_text, word_to_vec_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49477d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed967602",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06144d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_trunc(data, maxlen):\n",
    "    new_data = []\n",
    "    zero_vector = []\n",
    "    for _ in range(len(data[0][0])):\n",
    "        zero_vector.append(0.0)\n",
    "        \n",
    "    for sample in data:\n",
    "        if len(sample) > maxlen:\n",
    "            temp =sample[:maxlen]\n",
    "        elif len(sample) < maxlen:\n",
    "            temp=sample\n",
    "            additional_elems = maxlen - len(sample)\n",
    "            for _ in range(additional_elems):\n",
    "                temp.append(zero_vector)\n",
    "        else:\n",
    "            temp = sample\n",
    "        new_data.append(temp)\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d964ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pad_trunc(X_train, 64)\n",
    "X_val = pad_trunc(X_val, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b0d555de",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2f2b2d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nuros = 50\n",
    "model.add(LSTM(num_nuros, return_sequences=True, input_shape=(64, 100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0343a235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 64, 50)            30200     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64, 50)            0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 3200)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 18)                57618     \n",
      "=================================================================\n",
      "Total params: 87,818\n",
      "Trainable params: 87,818\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.add(Dropout(.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(18, activation=\"softmax\"))\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "         optimizer=\"sgd\",\n",
    "         metrics=\"accuracy\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfafe987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 50/154 [========>.....................] - ETA: 2s - loss: 2.8692 - accuracy: 0.0862"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_embed_matrix, y_train, batch_size=32, epochs=10, validation_data=(X_val_embed_matrix, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0b9fb2c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 64, 100)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val_embed_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddca33ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
