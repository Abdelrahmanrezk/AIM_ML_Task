{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7ce1444",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from configs import *\n",
    "from fetch_data import *\n",
    "from features_extraction import *\n",
    "from data_shuffling_split import *\n",
    "from data_preprocess import *\n",
    "from ml_modeling import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52b125bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of instances in the file are:  449033\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>dialect</th>\n",
       "      <th>dialect_l_encoded</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1056552188082716800</td>\n",
       "      <td>LY</td>\n",
       "      <td>8</td>\n",
       "      <td>توا دوشه الكلاسيكو شن بيتمها وشن بيسكتهم وشن ب...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>891734969202114560</td>\n",
       "      <td>SY</td>\n",
       "      <td>15</td>\n",
       "      <td>حسابشخصي في احلي من الشحاطه 😂</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1110565179257954432</td>\n",
       "      <td>SD</td>\n",
       "      <td>14</td>\n",
       "      <td>حسابشخصي موهبه والله 😂 اوع تحاول تطورها تقوم م...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1172817955270340608</td>\n",
       "      <td>LB</td>\n",
       "      <td>7</td>\n",
       "      <td>حسابشخصي حسابشخصي 😂 انا صرلي عشر سنين مش مجدده...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>293253217821790208</td>\n",
       "      <td>QA</td>\n",
       "      <td>12</td>\n",
       "      <td>احلي شعور تكون باجازه وتقوم من الصبح وتمر ع ال...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id dialect  dialect_l_encoded  \\\n",
       "0  1056552188082716800      LY                  8   \n",
       "1   891734969202114560      SY                 15   \n",
       "2  1110565179257954432      SD                 14   \n",
       "3  1172817955270340608      LB                  7   \n",
       "4   293253217821790208      QA                 12   \n",
       "\n",
       "                                                text  \n",
       "0  توا دوشه الكلاسيكو شن بيتمها وشن بيسكتهم وشن ب...  \n",
       "1                     حسابشخصي في احلي من الشحاطه 😂   \n",
       "2  حسابشخصي موهبه والله 😂 اوع تحاول تطورها تقوم م...  \n",
       "3  حسابشخصي حسابشخصي 😂 انا صرلي عشر سنين مش مجدده...  \n",
       "4  احلي شعور تكون باجازه وتقوم من الصبح وتمر ع ال...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strat_train_set = read_csv(\"train/strat_train_set.csv\")\n",
    "strat_train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8adb7157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of instances in the training data after StratifiedShuffleSplit are:  980\n",
      "The number of instances in the testing data after StratifiedShuffleSplit are:   20\n",
      "The number of trainin instances:  980\n",
      "The number of validation instances:  20\n",
      "The number of trainin labels :  980\n",
      "The number of validation labels :  20\n"
     ]
    }
   ],
   "source": [
    "x_train_text, x_val_text, y_train, y_val = prepare_data(strat_train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d443a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Tokenization : \n",
      " ['حسابشخصي الله يديم عليج السعاده يارب', 'حسابشخصي حق واجب عزكم عزنا 🇦🇪 الله لايفرقنا 💝 ', 'حسابشخصي اقول اقعد ع دراستكك احسنلك 🤚 ']\n",
      "==================================================\n",
      "After Tokenization : \n",
      " [['حسابشخصي', 'الله', 'يديم', 'عليج', 'السعاده', 'يارب'], ['حسابشخصي', 'حق', 'واجب', 'عزكم', 'عزنا', '🇦🇪', 'الله', 'لايفرقنا', '💝'], ['حسابشخصي', 'اقول', 'اقعد', 'ع', 'دراستكك', 'احسنلك', '🤚']]\n",
      "==================================================\n",
      "Before Tokenization : \n",
      " ['ايه ده , هو الاهلي ماوقعش عقوبه علي متعب فعلا , لا و الله , طيب فين المباديء , طيب فين الاخلاق , طيب اي حاجه مش كده 😂 ', 'حسابشخصي شكل مخك علبه سمنت كنز . . . ', 'حسابشخصي اقوئ اغنيه حب اتحدي العالم']\n",
      "==================================================\n",
      "After Tokenization : \n",
      " [['ايه', 'ده', ',', 'هو', 'الاهلي', 'ماوقعش', 'عقوبه', 'علي', 'متعب', 'فعلا', ',', 'لا', 'و', 'الله', ',', 'طيب', 'فين', 'المباديء', ',', 'طيب', 'فين', 'الاخلاق', ',', 'طيب', 'اي', 'حاجه', 'مش', 'كده', '😂'], ['حسابشخصي', 'شكل', 'مخك', 'علبه', 'سمنت', 'كنز', '.', '.', '.'], ['حسابشخصي', 'اقوئ', 'اغنيه', 'حب', 'اتحدي', 'العالم']]\n"
     ]
    }
   ],
   "source": [
    "x_train_text_tokenized = tokenize_using_nltk_TreebankWordTokenizer(x_train_text)\n",
    "\n",
    "print(\"Before Tokenization : \\n\", x_train_text[:3])\n",
    "print(\"=\"*50)\n",
    "print(\"After Tokenization : \\n\", x_train_text_tokenized[:3])\n",
    "print(\"=\"*50)\n",
    "\n",
    "x_val_text_tokenized = tokenize_using_nltk_TreebankWordTokenizer(x_val_text)\n",
    "\n",
    "print(\"Before Tokenization : \\n\", x_val_text[:3])\n",
    "print(\"=\"*50)\n",
    "print(\"After Tokenization : \\n\", x_val_text_tokenized[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b65ca08",
   "metadata": {},
   "source": [
    "# Curse of Dimensional & sparsity\n",
    "\n",
    "Tasks like **Computer Vision** or **Natural Language Processing** run to problem called **Curse of Dimensional**, and as we have here NLP classification problem, the number of instance are semi-large, but this not the point, the point is what we dealing with is text language, and the language are free of grammar, ritch of vocabulary and others.\n",
    "\n",
    "So to handle like these problems we need to extract features from the text, the old or classical way is using BOW (Bag of Words), and this approach run to the problem of **Curse of Dimensionality** as we will have number of features related to the unique words in our data. Not just that most of these features are zeros, what is we called sparse matrix.\n",
    "\n",
    "Beside of that, this matrix we will get from that approach represent the text not the word itself, so there is no similarity between words and other problem.\n",
    "\n",
    "# Word2Vec\n",
    "\n",
    "From what we have of these problem we moved to another approach related to the word representation.\n",
    "\n",
    "Word2vec is numerical representation of dense vector for the word semantics of meaning, including the implies meaning of the word. So we can use these word representation in our text as we will see.\n",
    "\n",
    "But to train Word2Vec and got a pretty good result of word representation, it first require massive data millions of text document, and second to wait for a while for your model to train. So we use the idea of transfer learning, and use some of the pre trained Arabic Word2Vec models and download it to use in our task. \n",
    "\n",
    "**Check for more information about the models we used:** [AraVec](https://www.sciencedirect.com/science/article/pii/S1877050917321749)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5de1eb",
   "metadata": {},
   "source": [
    "# Build Matrix of Text\n",
    "\n",
    "Any ML or DL model require specific number of features (input) to dealing with, but what we have here with word2vec is word representation. So how it works for text ?\n",
    "\n",
    "We will build a matrix for each text, but we need to limit the number of words in each text, because we can not train the model with different number of words in text.\n",
    "\n",
    "# Note !\n",
    "\n",
    "We can take the maximum number of words in the longest text, but maybe for some documents its has thousand of words, so we use the graph below and other helpful method to get reasonable length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7b574d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The max length is:  71\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[6, 9, 7, 16, 21, 12, 23, 9, 8, 40]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get how many words inside each text after tokenization\n",
    "num_of_words_in_each_text = [len(text) for text in x_train_text_tokenized]\n",
    "max_len = max(num_of_words_in_each_text)\n",
    "print(\"The max length is: \", max_len)\n",
    "num_of_words_in_each_text[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3442734c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of keys before removing are:  52\n",
      "==================================================\n",
      "The number of keys after removing some of them are:  0\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1332x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# count how many times these value repeated and sort them\n",
    "new_dicts = get_keys_that_val_gr_than_num(num_of_words_in_each_text, 1000)\n",
    "keys = list(new_dicts.keys())\n",
    "values = list(new_dicts.values())\n",
    "plt.style.use('dark_background')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(18.5, 6)\n",
    "plt.bar(range(len(new_dicts)), values, tick_label=keys)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65d666b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_vec_model = load_word2vec_model(\"models/word2vec/mohamed_w2v_CBOW_300_3_400_10/w2v_CBOW_300_3_400_10.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f12845c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of matrix (980, 19200)\n",
      "The shape of matrix (20, 19200)\n"
     ]
    }
   ],
   "source": [
    "number_of_features = 300\n",
    "max_len_str = 64\n",
    "X_train_embed_matrix = ML_text_to_matrix_using_word2vec(word_to_vec_model, x_train_text_tokenized, number_of_features, max_len_str)\n",
    "\n",
    "X_val_embed_matrix = ML_text_to_matrix_using_word2vec(word_to_vec_model, x_val_text_tokenized, number_of_features, max_len_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7fc860",
   "metadata": {},
   "source": [
    "# Train Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5b7cca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 out of  18 | elapsed:    8.5s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  18 out of  18 | elapsed:    8.5s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan {'C': 0.2, 'penalty': 'l1'}\n",
      "0.11632677310619562 {'C': 0.2, 'penalty': 'l2'}\n",
      "nan {'C': 0.5, 'penalty': 'l1'}\n",
      "0.11632677310619562 {'C': 0.5, 'penalty': 'l2'}\n",
      "nan {'C': 1, 'penalty': 'l1'}\n",
      "0.11632677310619562 {'C': 1, 'penalty': 'l2'}\n",
      "==================================================\n",
      "The best score is:  0.11632677310619562\n",
      "The best paramters for that score is:  {'C': 0.2, 'penalty': 'l2'}\n",
      "===================== Validate Result =====================\n",
      "F1 score is:  0.10000000000000002\n"
     ]
    }
   ],
   "source": [
    "lg_cls_parameters = {'penalty':['l1', 'l2'], 'C': [.2, .5, 1]}\n",
    "lg_cls = LogisticRegression(verbose=1)\n",
    "grid_s_model = grid_search(lg_cls, lg_cls_parameters, X_train_embed_matrix, y_train)\n",
    "_ = grid_search_result(grid_s_model)\n",
    "model = LogisticRegression(penalty=grid_s_model.best_params_['penalty'], C=grid_s_model.best_params_['C'])\n",
    "model_path_to_save = \"models/lg_cls_model.sav\"\n",
    "model = model_fit(model, X_train_embed_matrix, y_train, model_path_to_save)\n",
    "_ = f1_score_display(model, X_val_embed_matrix, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45551027",
   "metadata": {},
   "source": [
    "# ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b93e9d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   6 | elapsed:    6.1s remaining:   12.1s\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of   6 | elapsed:    9.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11632677310619562 {'max_depth': 3}\n",
      "0.11632677310619562 {'max_depth': 5}\n",
      "==================================================\n",
      "The best score is:  0.11632677310619562\n",
      "The best paramters for that score is:  {'max_depth': 3}\n",
      "===================== Validate Result =====================\n",
      "F1 score is:  0.10000000000000002\n"
     ]
    }
   ],
   "source": [
    "ex_tree_cls_parameters = {'max_depth':[3, 5]}\n",
    "ex_tree_cls = ExtraTreesClassifier(n_estimators=30,bootstrap=True, oob_score=True, min_samples_leaf=100)\n",
    "grid_s_model = grid_search(ex_tree_cls, ex_tree_cls_parameters, X_train_embed_matrix, y_train)\n",
    "_ = grid_search_result(grid_s_model)\n",
    "model = ExtraTreesClassifier(max_depth=grid_s_model.best_params_['max_depth'], n_estimators=30,bootstrap=True, oob_score=True, min_samples_leaf=100)\n",
    "model_path_to_save = \"models/ex_tree_model.sav\"\n",
    "model = model_fit(model, X_train_embed_matrix, y_train, model_path_to_save)\n",
    "_ = f1_score_display(model, X_val_embed_matrix, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d088a7",
   "metadata": {},
   "source": [
    "# Train SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "240589b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# svc_parameters = {'C':[.5, 1]}\n",
    "# svc_clf = SVC(kernel=\"linear\")\n",
    "# grid_s_model = grid_search(svc_clf, svc_parameters, X_train_embed_matrix, y_train)\n",
    "# _ = grid_search_result(grid_s_model)\n",
    "# model = SVC(kernel=\"linear\", C=grid_s_model.best_params_['C'])\n",
    "# model_path_to_save = \"models/SVC_model.sav\"\n",
    "# model = model_fit(model, X_train_embed_matrix, y_train, model_path_to_save)\n",
    "# _ = f1_score_display(model, X_val_embed_matrix, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd865d0",
   "metadata": {},
   "source": [
    "# Note !\n",
    "\n",
    "**We can use different word embedding representation, to see its effect on training.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3d0b62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
