{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7ce1444",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier \n",
    "from sklearn.ensemble import BaggingClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from configs import *\n",
    "from fetch_data import *\n",
    "from features_extraction import *\n",
    "from data_shuffling_split import *\n",
    "from data_preprocess import *\n",
    "from ml_modeling import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52b125bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of instances in the file are:  449033\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>dialect</th>\n",
       "      <th>dialect_l_encoded</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1056552188082716800</td>\n",
       "      <td>LY</td>\n",
       "      <td>8</td>\n",
       "      <td>توا دوشه الكلاسيكو شن بيتمها وشن بيسكتهم وشن ب...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>891734969202114560</td>\n",
       "      <td>SY</td>\n",
       "      <td>15</td>\n",
       "      <td>حسابشخصي في احلي من الشحاطه 😂</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1110565179257954432</td>\n",
       "      <td>SD</td>\n",
       "      <td>14</td>\n",
       "      <td>حسابشخصي موهبه والله 😂 اوع تحاول تطورها تقوم م...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1172817955270340608</td>\n",
       "      <td>LB</td>\n",
       "      <td>7</td>\n",
       "      <td>حسابشخصي حسابشخصي 😂 انا صرلي عشر سنين مش مجدده...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>293253217821790208</td>\n",
       "      <td>QA</td>\n",
       "      <td>12</td>\n",
       "      <td>احلي شعور تكون باجازه وتقوم من الصبح وتمر ع ال...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id dialect  dialect_l_encoded  \\\n",
       "0  1056552188082716800      LY                  8   \n",
       "1   891734969202114560      SY                 15   \n",
       "2  1110565179257954432      SD                 14   \n",
       "3  1172817955270340608      LB                  7   \n",
       "4   293253217821790208      QA                 12   \n",
       "\n",
       "                                                text  \n",
       "0  توا دوشه الكلاسيكو شن بيتمها وشن بيسكتهم وشن ب...  \n",
       "1                     حسابشخصي في احلي من الشحاطه 😂   \n",
       "2  حسابشخصي موهبه والله 😂 اوع تحاول تطورها تقوم م...  \n",
       "3  حسابشخصي حسابشخصي 😂 انا صرلي عشر سنين مش مجدده...  \n",
       "4  احلي شعور تكون باجازه وتقوم من الصبح وتمر ع ال...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strat_train_set = read_csv(\"train/strat_train_set.csv\")\n",
    "strat_train_set = strat_train_set.iloc[:10000]\n",
    "strat_train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8adb7157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of instances in the training data after StratifiedShuffleSplit are:  9800\n",
      "The number of instances in the testing data after StratifiedShuffleSplit are:   200\n",
      "The number of trainin instances:  9800\n",
      "The number of validation instances:  200\n",
      "The number of trainin labels :  9800\n",
      "The number of validation labels :  200\n"
     ]
    }
   ],
   "source": [
    "x_train_text, x_val_text, y_train, y_val = prepare_data(strat_train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d443a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Tokenization : \n",
      " ['حسابشخصي هذا مسوي بلوك لنص الشعب لقطري ويتكلم عنه ! ! ! ! والله ان فرقاهم عيد . . . هم بالذات . #قرقاش', 'حسابشخصي حسابشخصي بالعكس الرحله وياهم احلا شي 😊 ونااسه', 'نقدر سهرهم والتعب معوضين في المره القادمه رابطويب']\n",
      "==================================================\n",
      "After Tokenization : \n",
      " [['حسابشخصي', 'هذا', 'مسوي', 'بلوك', 'لنص', 'الشعب', 'لقطري', 'ويتكلم', 'عنه', '!', '!', '!', '!', 'والله', 'ان', 'فرقاهم', 'عيد', '.', '.', '.', 'هم', 'بالذات', '.', '#', 'قرقاش'], ['حسابشخصي', 'حسابشخصي', 'بالعكس', 'الرحله', 'وياهم', 'احلا', 'شي', '😊', 'ونااسه'], ['نقدر', 'سهرهم', 'والتعب', 'معوضين', 'في', 'المره', 'القادمه', 'رابطويب']]\n",
      "==================================================\n",
      "Before Tokenization : \n",
      " ['حسابشخصي الله الله قريب ، لازم زياره ليه بعد رمضان', 'حسابشخصي يا ابني ارحمي بقي', '#برت اثاري فوزهم علينا بهالطريقه ؟ ؟ ']\n",
      "==================================================\n",
      "After Tokenization : \n",
      " [['حسابشخصي', 'الله', 'الله', 'قريب', '،', 'لازم', 'زياره', 'ليه', 'بعد', 'رمضان'], ['حسابشخصي', 'يا', 'ابني', 'ارحمي', 'بقي'], ['#', 'برت', 'اثاري', 'فوزهم', 'علينا', 'بهالطريقه', '؟', '؟']]\n"
     ]
    }
   ],
   "source": [
    "x_train_text_tokenized = tokenize_using_nltk_TreebankWordTokenizer(x_train_text)\n",
    "\n",
    "print(\"Before Tokenization : \\n\", x_train_text[:3])\n",
    "print(\"=\"*50)\n",
    "print(\"After Tokenization : \\n\", x_train_text_tokenized[:3])\n",
    "print(\"=\"*50)\n",
    "\n",
    "x_val_text_tokenized = tokenize_using_nltk_TreebankWordTokenizer(x_val_text)\n",
    "\n",
    "print(\"Before Tokenization : \\n\", x_val_text[:3])\n",
    "print(\"=\"*50)\n",
    "print(\"After Tokenization : \\n\", x_val_text_tokenized[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b65ca08",
   "metadata": {},
   "source": [
    "# Curse of Dimensional & sparsity\n",
    "\n",
    "Tasks like **Computer Vision** or **Natural Language Processing** run to problem called **Curse of Dimensional**, and as we have here NLP classification problem, the number of instance are semi-large, but this not the point, the point is what we dealing with is text language, and the language are free of grammar, ritch of vocabulary and others.\n",
    "\n",
    "So to handle like these problems we need to extract features from the text, the old or classical way is using BOW (Bag of Words), and this approach run to the problem of **Curse of Dimensionality** as we will have number of features related to the unique words in our data. Not just that most of these features are zeros, what is we called sparse matrix.\n",
    "\n",
    "Beside of that, this matrix we will get from that approach represent the text not the word itself, so there is no similarity between words and other problem.\n",
    "\n",
    "# Word2Vec\n",
    "\n",
    "From what we have of these problem we moved to another approach related to the word representation.\n",
    "\n",
    "Word2vec is numerical representation of dense vector for the word semantics of meaning, including the implies meaning of the word. So we can use these word representation in our text as we will see.\n",
    "\n",
    "But to train Word2Vec and got a pretty good result of word representation, it first require massive data millions of text document, and second to wait for a while for your model to train. So we use the idea of transfer learning, and use some of the pre trained Arabic Word2Vec models and download it to use in our task. \n",
    "\n",
    "**Check for more information about the models we used:** [AraVec](https://www.sciencedirect.com/science/article/pii/S1877050917321749)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5de1eb",
   "metadata": {},
   "source": [
    "# Build Matrix of Text\n",
    "\n",
    "Any ML or DL model require specific number of features (input) to dealing with, but what we have here with word2vec is word representation. So how it works for text ?\n",
    "\n",
    "We will build a matrix for each text, but we need to limit the number of words in each text, because we can not train the model with different number of words in text.\n",
    "\n",
    "# Note !\n",
    "\n",
    "We can take the maximum number of words in the longest text, but maybe for some documents its has thousand of words, so we use the graph below and other helpful method to get reasonable length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7b574d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The max length is:  80\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[25, 9, 8, 10, 23, 9, 17, 11, 22, 9]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get how many words inside each text after tokenization\n",
    "num_of_words_in_each_text = [len(text) for text in x_train_text_tokenized]\n",
    "max_len = max(num_of_words_in_each_text)\n",
    "print(\"The max length is: \", max_len)\n",
    "num_of_words_in_each_text[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3442734c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of keys before removing are:  72\n",
      "==================================================\n",
      "The number of keys after removing some of them are:  0\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1332x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# count how many times these value repeated and sort them\n",
    "new_dicts = get_keys_that_val_gr_than_num(num_of_words_in_each_text, 1000)\n",
    "keys = list(new_dicts.keys())\n",
    "values = list(new_dicts.values())\n",
    "plt.style.use('dark_background')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(18.5, 6)\n",
    "plt.bar(range(len(new_dicts)), values, tick_label=keys)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65d666b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_vec_model = load_word2vec_model(\"models/word2vec/bakrianoo_unigram_cbow_100_twitter/full_uni_cbow_100_twitter.mdl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f12845c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9800, 6400)\n",
      "==================================================\n",
      "[ 0.6406  -1.85     0.01985 -0.8535   0.3623  -2.416   -1.094   -2.168\n",
      "  2.006   -2.81     1.923    0.06726  2.766   -3.68    -1.073   -1.608\n",
      " -0.6113   1.991    2.984    2.396   -2.871    0.1931   0.777    3.166\n",
      "  3.594    1.499   -0.5244   2.85     1.408   -0.753    0.717   -2.047\n",
      " -0.5664   3.615   -0.06015  3.229   -2.357    4.137    1.76    -3.941\n",
      " -3.717   -0.2527  -0.2693  -2.22     1.606    3.387   -0.11206  1.086\n",
      "  2.904   -1.972  ]\n",
      "(200, 6400)\n",
      "==================================================\n",
      "[ 5.44    -1.274    3.229   -3.773   -3.795    4.926   -0.6597   2.365\n",
      "  0.5625  -3.352    0.3213   0.5005  -3.709   -5.33     3.615    1.654\n",
      "  2.988   -0.7603   0.3733   2.418   -2.904   -2.7      2.135   -0.3982\n",
      "  0.63     1.314   -0.8735   4.855    4.684   -1.277    3.67    -2.932\n",
      "  2.867    2.352    2.062   -0.756   -2.1      5.863    0.9263  -0.7715\n",
      "  0.5312   4.836   -0.761   -0.4324  -3.611   -1.574    0.10724 -3.316\n",
      " -1.546    3.238  ]\n"
     ]
    }
   ],
   "source": [
    "number_of_features = 100\n",
    "max_len_str = 64\n",
    "word2vec_path = \"bakr/\"\n",
    "model_path_to_save = \"models/ml_models/\"\n",
    "estimators = voting_models()\n",
    "\n",
    "X_train_embed_matrix = text_to_matrix_using_word2vec(word_to_vec_model, x_train_text_tokenized, max_len_str)\n",
    "X_val_embed_matrix = text_to_matrix_using_word2vec(word_to_vec_model, x_val_text_tokenized, max_len_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7fc860",
   "metadata": {},
   "source": [
    "# Train Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5b7cca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================== Validate Result =====================\n",
      "F1 score is:  0.235\n",
      "It takes to run:  0:00:37.374069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abdelrahman/.local/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   36.1s finished\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(penalty='l2', C=1, multi_class='multinomial', solver='lbfgs', verbose=1)\n",
    "model = ml_classifer_pipeline(model, X_train_embed_matrix, y_train, X_val_embed_matrix, y_val,word2vec_path, model_path_to_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d088a7",
   "metadata": {},
   "source": [
    "# Train SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "240589b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]===================== Validate Result =====================\n",
      "F1 score is:  0.175\n",
      "It takes to run:  0:02:20.763607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abdelrahman/.local/lib/python3.6/site-packages/sklearn/svm/_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "model = LinearSVC(C=0.5,  verbose=1)\n",
    "model = ml_classifer_pipeline(model, X_train_embed_matrix, y_train, X_val_embed_matrix, y_val,word2vec_path, model_path_to_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ef9bae",
   "metadata": {},
   "source": [
    "# Hard Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37c04c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abdelrahman/.local/lib/python3.6/site-packages/sklearn/svm/_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/home/abdelrahman/.local/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   32.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================== Validate Result =====================\n",
      "F1 score is:  0.20999999999999996\n",
      "It takes to run:  0:02:57.843415\n"
     ]
    }
   ],
   "source": [
    "model = VotingClassifier(estimators, voting=\"hard\")\n",
    "model = ml_classifer_pipeline(model, X_train_embed_matrix, y_train, X_val_embed_matrix, y_val,word2vec_path, model_path_to_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e6ab06",
   "metadata": {},
   "source": [
    "# Extr Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b56fbb6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================== Validate Result =====================\n",
      "F1 score is:  0.125\n",
      "It takes to run:  0:00:05.345713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    4.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "model = ExtraTreesClassifier(n_estimators=100, max_depth=5, max_samples=.1, verbose=1)\n",
    "model = ml_classifer_pipeline(model, X_train_embed_matrix, y_train, X_val_embed_matrix, y_val,word2vec_path, model_path_to_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188d2aa4",
   "metadata": {},
   "source": [
    "# AdaBoost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75f861c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][LibLinear][LibLinear][LibLinear][LibLinear]===================== Validate Result =====================\n",
      "F1 score is:  0.20000000000000004\n",
      "It takes to run:  0:01:03.547857\n"
     ]
    }
   ],
   "source": [
    "model = LinearSVC(C=0.5,  verbose=1)\n",
    "model = AdaBoostClassifier(model,  algorithm=\"SAMME\", n_estimators=5)\n",
    "model = ml_classifer_pipeline(model, X_train_embed_matrix, y_train, X_val_embed_matrix, y_val,word2vec_path, model_path_to_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693af6be",
   "metadata": {},
   "source": [
    "#  Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43bc7ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1        1829.4357      -13114.4928            2.42m\n",
      "         2        2422.3346 -1093570909.1189            2.16m\n",
      "         3    29001179.7258 -10636194716047820.0000            1.83m\n",
      "         4 2126700032841648.7500 -1316898463890755.0000            1.57m\n",
      "         5 1157467509366.6533 -7099516442538.0000            1.31m\n",
      "         6 328615268297702.1250 -16508472307225218.0000            1.05m\n",
      "         7 8571820817040256.0000 -286123861372164.0000           47.30s\n",
      "         8 1529202155651900.0000 -27797984350528.0000           31.77s\n",
      "         9 1485625839361327.7500 -3203648492716.0000           15.94s\n",
      "        10 3590186887549460.0000 -21596092330969060.0000            0.00s\n",
      "===================== Validate Result =====================\n",
      "F1 score is:  0.095\n",
      "It takes to run:  0:02:38.593899\n"
     ]
    }
   ],
   "source": [
    "model = GradientBoostingClassifier(n_estimators=10, subsample=.1, learning_rate=.5,   max_depth=5, verbose=1)\n",
    "model = ml_classifer_pipeline(model, X_train_embed_matrix, y_train, X_val_embed_matrix, y_val,word2vec_path, model_path_to_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0347e6",
   "metadata": {},
   "source": [
    "# XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c5e08de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abdelrahman/.local/lib/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:51:37] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "===================== Validate Result =====================\n",
      "F1 score is:  0.155\n",
      "It takes to run:  0:16:52.301682\n"
     ]
    }
   ],
   "source": [
    "model = XGBClassifier(max_depth=5, subsample=.1)\n",
    "model = ml_classifer_pipeline(model, X_train_embed_matrix, y_train, X_val_embed_matrix, y_val,word2vec_path, model_path_to_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba93151",
   "metadata": {},
   "source": [
    "# Rezk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0bb7004",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_vec_model = load_word2vec_model(\"models/word2vec/rezk_unigram_CBOW_model/train_word2vec_cbow__window_3_min_count_300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5170aec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9800, 19200)\n",
      "==================================================\n",
      "[-0.1262   0.2761   0.2466  -0.3464  -0.5044   0.216    0.2651   0.05423\n",
      " -0.3276  -0.2793   0.328    0.1699  -0.05267  0.1941   0.292    0.1654\n",
      " -0.01619 -0.428    0.411    0.0927   0.271    0.6206  -0.04764  0.04465\n",
      "  0.0863   0.06042  0.08374 -0.0927   0.05176 -0.1616  -0.4875   0.4932\n",
      "  0.1333   0.4666   0.0387  -0.19     0.05563 -0.1526   0.549    0.2966\n",
      " -0.0969  -0.345   -0.2896  -0.0667   0.12146  0.2126   0.1146  -0.4404\n",
      " -0.1198   0.2651 ]\n",
      "(200, 19200)\n",
      "==================================================\n",
      "[-0.1262   0.2761   0.2466  -0.3464  -0.5044   0.216    0.2651   0.05423\n",
      " -0.3276  -0.2793   0.328    0.1699  -0.05267  0.1941   0.292    0.1654\n",
      " -0.01619 -0.428    0.411    0.0927   0.271    0.6206  -0.04764  0.04465\n",
      "  0.0863   0.06042  0.08374 -0.0927   0.05176 -0.1616  -0.4875   0.4932\n",
      "  0.1333   0.4666   0.0387  -0.19     0.05563 -0.1526   0.549    0.2966\n",
      " -0.0969  -0.345   -0.2896  -0.0667   0.12146  0.2126   0.1146  -0.4404\n",
      " -0.1198   0.2651 ]\n"
     ]
    }
   ],
   "source": [
    "number_of_features = 300\n",
    "max_len_str = 64\n",
    "word2vec_path = \"rezk/\"\n",
    "model_path_to_save = \"models/ml_models/\"\n",
    "estimators = voting_models()\n",
    "\n",
    "X_train_embed_matrix = text_to_matrix_using_word2vec(word_to_vec_model, x_train_text_tokenized, max_len_str)\n",
    "X_val_embed_matrix = text_to_matrix_using_word2vec(word_to_vec_model, x_val_text_tokenized, max_len_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d496826",
   "metadata": {},
   "source": [
    "# Train Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c8d7d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================== Validate Result =====================\n",
      "F1 score is:  0.275\n",
      "It takes to run:  0:01:27.296875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abdelrahman/.local/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.4min finished\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(penalty='l2', C=1, multi_class='multinomial', solver='lbfgs', verbose=1)\n",
    "model = ml_classifer_pipeline(model, X_train_embed_matrix, y_train, X_val_embed_matrix, y_val,word2vec_path, model_path_to_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609cb3ea",
   "metadata": {},
   "source": [
    "# Train SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b277bf9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]===================== Validate Result =====================\n",
      "F1 score is:  0.24\n",
      "It takes to run:  0:03:21.084786\n"
     ]
    }
   ],
   "source": [
    "model = LinearSVC(C=0.5,  verbose=1)\n",
    "model = ml_classifer_pipeline(model, X_train_embed_matrix, y_train, X_val_embed_matrix, y_val,word2vec_path, model_path_to_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad287fd",
   "metadata": {},
   "source": [
    "# Bagging Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0843f619",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "dec_tree_cls = DecisionTreeClassifier(max_depth=5, min_samples_split=2, min_samples_leaf=1)\n",
    "model = BaggingClassifier(base_estimator=dec_tree_cls, n_estimators=100, max_samples=.2, verbose=1)\n",
    "ml_classifer_pipeline(model, X_train_embed_matrix, y_train, X_val_embed_matrix, y_val,word2vec_path, model_path_to_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c824b240",
   "metadata": {},
   "source": [
    "# Extr Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ef2a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ExtraTreesClassifier(n_estimators=100, max_depth=5, max_samples=.1, verbose=1)\n",
    "model = ml_classifer_pipeline(model, X_train_embed_matrix, y_train, X_val_embed_matrix, y_val,word2vec_path, model_path_to_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2fc3aa",
   "metadata": {},
   "source": [
    "# AdaBoost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c5b628",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearSVC(C=0.5,  verbose=1)\n",
    "model = AdaBoostClassifier(model,  algorithm=\"SAMME\", n_estimators=5)\n",
    "model = ml_classifer_pipeline(model, X_train_embed_matrix, y_train, X_val_embed_matrix, y_val,word2vec_path, model_path_to_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c07f3fd",
   "metadata": {},
   "source": [
    "#  Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686488f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GradientBoostingClassifier(n_estimators=10, subsample=.1, learning_rate=.5,   max_depth=5, verbose=1)\n",
    "model = ml_classifer_pipeline(model, X_train_embed_matrix, y_train, X_val_embed_matrix, y_val,word2vec_path, model_path_to_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e40b05",
   "metadata": {},
   "source": [
    "# XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56be6d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBClassifier(max_depth=5, subsample=.1)\n",
    "model = ml_classifer_pipeline(model, X_train_embed_matrix, y_train, X_val_embed_matrix, y_val,word2vec_path, model_path_to_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd865d0",
   "metadata": {},
   "source": [
    "# Note !\n",
    "\n",
    "**We can use different word embedding representation, to see its effect on training.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5671d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
